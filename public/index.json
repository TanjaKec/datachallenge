[
{
	"uri": "/general/whatlearn/",
	"title": "General Overview",
	"tags": [],
	"description": "",
	"content": " We live in a digital world where having access to the information at our fingertips doesn’t make it easier to communicate… it makes it harder. In every data bundle there are many concealed narratives and conclusions, some stories will be unremarkable, others will be highly charged, but all remain out of sight without the skills required to view the hidden truths. Revealing unseen tales within data bundles and communicating that information requires curiosity, determination and skill. Applying the basic concepts, tools and related know how of data science procedures is key to deciphering and revealing the headlining facts. Naturally, in order for this to happen the basic concepts and practices of data analysis need to be applied and adopted by those wishing to uncover the stories. Telling stories with data is a skill that’s becoming ever more important in the digital world of increasing data and desire for informed data driven decision making.\nThe most viable software of choice for reporting modern data analysis is R, as data and code is reproducible and therefore useful to others, as all the information collated within the analysis is available. R is the lingua franca of quantitative research and as such is an influential and indispensable feature of any data scrutiny process. Through the use of grammar of graphics in R and Shiny, a web application framework for R, it is possible to develop interactive applications for graphical data visualisation. Presenting findings through interactive and accessible visualisation methods is an effective form of reaching audiences that might not otherwise realise the value of the topic or data at hand.\nThis course will provide an overview of key concepts for creating an effective data science project and will introduce tools and techniques for data wrangling, visualisation and dynamic reproducible reporting using R, a public domain language for data analysis. The R language provides a rich and flexible environment for working with data, especially data to be used for statistical modelling or graphics.\nThe R system has an extensive library of packages that offer state-of-the-art-abilities. Many of the analyses that they offer are not even available in any of the standard packages. R enables you to escape from the restrictive environments and sterile analyses offered by commonly used statistical software packages. It enables easy experimentation and exploration, which improves data analysis. Sharing your data analysis knowledge discovery is necessary in making it useful. R is a tool that enables reporting modern data analyses in a reproducible manner. It makes analysis more useful to others because the data and code that actually conducted the analysis can be made available and easily shared. As such R has become the lingua franca of quantitative research. Accordingly, this course will emphasize packages that will help you do data analysis, visualization and communication with the wider audience.\nThe course will start by introducing the fundamental concepts of R: basic use of R console through RStudio IDE, inputting and importing data, record keeping and general good practice of R project workflow. It will then progress to basic statistical concepts, which theoretically may be perceived as complex and thereby can be more effectively communicated by using visualisation. Hence, the formal abstract nature of Statistics can be demystified by visualising its application context, which is why the focus is directed on building appropriate visualisation of a given data analysis problem. At the end of the course, after students develop an understanding of their data they will use R’s reproducible and interactive approach to knit this into a tight and concise narrative and of course, present their story by creating reproducible RMarkdown documents and Shiny Web Apps.\nVersion control has become an essential tool for keeping track when working on DS projects, as well as collaborating. RStudio supports working with Git, an open source distributed version control system, which is easy to use when combined with GitHub, a web-based Git repository hosting service. We will introduce you to GitHub and you’ll become acquainted with good practice when incorporating the use of Git into your R project workflow.\nThere is a demand for open and transparent data sources by governments and civic groups as a means to improve the lives of citizens. Together we will investigate the importance of open source data and we will identify where open source data can be readily found accross the Internet. You will work on case studies inspired by real problems and based on open data.\nObjectives:  To learn how to access and prepare data for the analysis\n To introduce the basic principles behind effective data visualization\n To learn essential explanatory techniques for summarising data\n To produce explanatory data visualizations providing insight into what could be found within the data\n To utilise R’s library of tools to visualise geospatial problems\n To design reproducible reports by automating the reporting process\n To share the results of analysis as interactive, eye-catching web apps that are friendly to non-programmers.\n To be familiar with R/RStudio’s data handling facilities that will expand the range of data analysis problems that can be effectively analysed.\n  How the course works The material is structured within 3 daily modules. Each module is a three and a halph hour long session split into 2½ hours hands on interactive student/teacher workshops with the last hour reserved for questions and discussions.\nEach module will be taught by Dr Tatjana Kecojevic and will cover various related topics through appropriate case studies, presentations and readings. The conceptual models come to life when practice becomes reality during the hands on taught sessions, through the application of R. Students are then expected to use their own time to practice and hone acquired data handling expertise acquired during the taught sessions.\nStudents are expected to participate fully in all of these delivery modes, but in particular are expected to have attempted any pre-set work and come fully prepared to discuss any problems encountered and debate the ideas and any issues raised.\nWe recommend you complete each of the following before the end of each module:\n Readings and hand-outs/exercises Participation in the discussions Exercises covering concepts from tutorials and/or readings   Who can enrol This course is design for anyone who needs to communicate information to someone using data. It will benefit anyone who has the curiosity and desire to enter the realm of data exploration. We will seek to make sense of the world of data and learn effective and attractive ways to visually analyse and communicate related information. With the knowledge gained in this course, you will be ready to undertake your very first explanatory data analysis.\nData Science is not simply fashionable jargon, but rather a discipline with a set of tools that empower data enriched living, so whatever industry you’re in, this is relevant to you!\nPrior experience is not required.\nThe course will be delivered in English and Serbian!\n© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/whatisr/",
	"title": "What is R?",
	"tags": [],
	"description": "",
	"content": " Before there was R, there was S! 🤠 R is a dialect of S language that was developed in 1976 by Rick Becker and John Chambers at the Bell Laboratories.\nRick Becker gave an excellent keynote talk \u0026ldquo;Forty Years of S\u0026rdquo; at UseR!2016 conference:\nRick Becker @ UseR!2016 where he talked about development of S language that gives explanations for many characteristics of R as we know it, including \u0026ldquo;\u0026lt;-\u0026rdquo; assignment operator.\n1993 Bell Labs gave StatSci (later Insightful Corp.) an exclusive license to develop and sell the S language. Insightful sold its implementation of the S language under the product name S-PLUS.\nYou can read more about the history of S, R, and S-PLUS\nThen, R was born 😇🎶 In the early nineties at the University of Auckland in the Department of Statistics R was created by Ross Ihaka and Robert Gentleman.\nThey used GNU General Public License to make R open source free software.\nRoss Ihaka and Robert Gentleman. R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5(3):299–314, 1996\nCurrently R is developed by the R Development Core Team, of which John Chambers is a member.\nTo start using R you need to install it! 😃\n© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/general/",
	"title": "About the course",
	"tags": [],
	"description": "",
	"content": " About the Course\nThis course is designed to give you an appreciation of R programming as a tool for data exploration. It focuses on packages that will help you do exploratory data analysis and visualization and communication in a dynamic and reproducible manner. If you would like to:  Discover how to find and access data and prepare it for exploration and visualization Learn to explore, visualize, and analyse data in a dynamic and reproducible manner Gain experience in data wrangling, exploratory data analysis and data visualization, and effective communication of results; Work on the case studies inspired by real problems and based on open data  then this course is for you! 😀 © 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/installr/",
	"title": "Install R/RStudio",
	"tags": [],
	"description": "",
	"content": " Install R To begin our journey to data science in R we need to install and make sure we can run it.\nWe will start by installing R first and then RStudio. The analogy we can use here is that R is an airplane and R Studio is its airport.\n R homepage - http://www.r-project.org CRAN Mirrors - http://cran.r-project.org  Run the installation\nYour R Counsole should look something like this:\nInstall RStudio RStudio is available in open source and commercial editions and runs on the desktop (Windows, Mac, and Linux).\nRStudio Desktop - http://www.rstudio.com\nRStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and work-space management. You can check its features by visiting RSTudio website.\nWe recomend you check regularly for R/Rstudio updates.\n © 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/general/instructor/",
	"title": "Meet the Instructor",
	"tags": [],
	"description": "",
	"content": "The instructor is Dr Tatjana Kecojević who is a longtime R user with a doctorate in Statistics from the University of Manchester. She spent many years working in the U.K. university sector as a senior lecturer and has published an extensive number of articles and papers in the field of quantile regression. Tatjana is the founder and co-organizer of R-Ladies Manchester, Belgrade and Novi Sad Chapters, leader of the R Forwards team and recently invited to become an ambassador for Women in DS (WiDS) ambassador. She is currently the founder and director of SisterAnalyst.org, an organisation aiming to empower women from a diverse range of backgrounds through data literacy. Unsurprisingly, Tatjana is a enthusiastic R user and in addition to her involvement supporting women in STEM related activities, she is dedicated to creating an inclusive culture by developing initiatives supporting all underrepresented groups within the DS community.\n© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/",
	"title": "Day 1",
	"tags": [],
	"description": "",
	"content": " Module 1 RStudio IDE; R language; Data classification and summary statistics; Introduction to Visualization Principles In this module you will set up the working environment and pass the first big hurdle of importing data and you will learn how to do it in the proper way with a command in R. You will learn how to use RStudio IDE for R from its installation to RStudio customisation and files navigation. You will learn good habits and practice of workflow in an R project. Once you get comfortable with the RStudio working environment you will move on to mastering the key features of R language and you will be introduced to fundamental principles behind effective data visualisation. What you will learn:  Basic use of R/RStudio console Good habits for workflow Inputting and importing different data types R environment: record keeping Data classification Descriptive summary statistics Basic principles of effective data visualisation  © 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/general/syllabus/",
	"title": "Indicative Syllabus",
	"tags": [],
	"description": "",
	"content": " Day 1 RStudio IDE; R language; Data classification and summary statistics; Introduction to Visualization Principles\nIn this module you will set up the working environment and pass the first big hurdle of importing data and you will learn how to do it in the proper way with a command in R. You will learn how to use RStudio IDE for R from its installation to RStudio customisation and files navigation. You will learn good habits and practice of workflow in an R project. Once you get comfortable with the RStudio working environment you will move on to mastering the key features of R language and you will be introduced to fundamental principles behind effective data visualisation.\nWhat you will learn:\n Basic use of R/RStudio console Good habits for workflow Inputting and importing different data types R environment: record keeping Data classification Descriptive summary statistics Basic principles of effective data visualisation  Day 2 Data Wrangling and Visualising Data\nIn this module you will learn some of the fundamental techniques for data exploration and transformation through the use of the dplyr package. This tidy verse package helps make your exploration intuitive to write and easy to read. You will learn dplyr’s key verbs for data manipulation that will help you uncover and shape the information within the data that is easy to turn into informative plots. Through the use of grammar of graphics plotting concepts implemented in the ggplot2 package, you will be able create meaningful exploratory plots. You will develop understanding about the way in which you should be able to think about necessary data transformations and summaries that can lead to an informative visualisation. You will learn how to create static maps and interactive maps with geolocated data by using the most popular packages in the R GIS community: simple features and leaflet.\nWhat you will learn:\n dplyr’s key data manipulation verbs: select, mutate, filter, arrange and summarise/summarize to aggregate data by groups to chain data manipulation operations using the pipe operator to specify ggplot2 building blocks and combine them to create graphical display about the philosophy that guides ggplot2: grammatical elements (layers) and aesthetic mappings. visualising data with maps  Day 3 Automated Reporting and Introduction to Shiny\nIn this module you will learn how to turn your analyses into high quality documents and presentations with R Markdown. You will be designing reproducible reports by automating the reporting process, learning how to take a modern approach to telling your data story. With the knowledge from this lesson you will be able to create reports straight from your R code allowing you to document your analysis and its results as an HTML, pdf, slideshow or Microsoft Word document. Afetr you gain fundamental knowledge of markdown and knitr, you will learn to create interactive web-graphics using Shiny R package.\nWhat you will learn:\n Authoring R Markdown Reports Embedding R Code knitr to compile dynamic R code LaTex to incorporate mathematical expressions create dynamic graphics using Shiny‘s reactive features build and deploy Shiny app  © 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/rstudioide/",
	"title": "RStudio IDE",
	"tags": [],
	"description": "",
	"content": " RStudio\u0026rsquo;s layout When you open RStudio for the first time it will be split into three sections. Each section has its own tab with shortcuts for the relevant options available from the main RStudio menu.\n The tall red section on the left is the Console and that’s where you can type in R code to execute. This code is also called commands or functions.\n In the top right section, there’s the Environment tab where you can see the data you are currently working on. At first this section is empty because you have not loaded any data yet.\n In the bottom right section there are tabs to flip through the Files and folder structure of your computer (like in Finder or Explorer), Help information etc.\n  🤓💡: You might find it useful bookmarking the link for RStudio IDE Cheet Sheet!\n © 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/user/",
	"title": "How to use R?",
	"tags": [],
	"description": "",
	"content": " The RStudio window has mutiple panes. RStudio IDE Cheet Sheet:\nTop Left - Code Editor: for creating scripts to run in pieces or as a whole (like this document!);\nBottom Left - R Console: you can type R commands and see output;\nTop Right - Environment: lists the objects that you create, such as data sets;\nBottom Right - Help: find out information about functions and packages. This same panel will have tabs for showing plots that you make, view apps and documents, show files in the folder, and packages used.\nR is a scripting language, which means that it is just like writing an essay, or a math proof. We write a script to do specific tasks, that we can run again and again, or give to someone else to run.\nYou should take note of the following facts:\n R is case-sensitive language which executes instructions directly; Commands are entered at prompt \u0026gt; Commands are separate statements which could be put the in a same line if separated by a semi-colon ; Code statements can be commented by using a # tag. You can comment in continuation of the command line or in a separate line.  💡 The best way to learn is by doing. Therefore, I would like you not to copy paste the commands showing in the black boxes, but to type it in your R console. It would be even better to type it as an R script, so that you can keep a history of it and return to it if you wish to do so.\n Let\u0026rsquo;s start using R!  Turn on your computer (if you haven\u0026rsquo;t already) Open RStudio Create a project for your work. On the very right side of the window is a small blue R, and a drop-down menu. Select New project, then New directory, navigate to the desktop, and name the project My_First_R. This will create a folder with this name on the desktop. This will be your workspace for this project. 😇  Setting up your working directory If you want to read or write files on your computer from and to a specific location you will need to set a working directory in R. To set the working directory in R to a specific folder on your computer you will use the following:\n# On a pc, you would set working directory like this setwd(\u0026quot;C:/Documents/MyR_Project\u0026quot;) # On a mac, you would set working directory like this setwd(\u0026quot;~/documents/MyR_Project\u0026quot;)  🤓💡: Make sure you fully adopt the correct syntax in terms of slashes and quotation marks.\n Note that the current working directory is displayed by RStudio within the title region of the Console. You can also setup your working directory by:\n selecting the options available from RStudio\u0026rsquo;s main menu  Use the Tools | Change Working Dir\u0026hellip; menu (Session | Set Working Directory on a mac).\n selecting the option from within the Files pane  Use the More | Set As Working Directory menu\nHowever, you should always start a fresh project (File | New Project\u0026hellip;) that would automatically set up your working directory without having to point it out to it in your script file. You should read Project-oriented workflow 💻🔥 article by Jenny Bryan to convince yourself that this would be a good habit you should adopt.\nR Packages \u0026ldquo;In R, the fundamental unit of shareable code is the package.\u0026rdquo; Hadley Wickham, R packages\nR packages are collections of functions code, data sets, documentation and tests developed by the community, that are mostly made available on the Comprehensive R Archive Network, or CRAN, the public clearing house for R packages. Those pachages are developed by the experts in their fields and currently the CRAN package repository features over 14,000 of them. Many of the analyses that they offer are not even available in any of the standard data analysis software packages, which is one of the reasons that R is so successful.\nWhen you run R you will authomatically upload the package:base, which is the system library, i.e. the package where all standard functions are defined. The rest of the so called base packages contain the basic statistical routines. Assuming that you are connected to the internet, you can install a package using install.packages(). From RStudio menu you can do it by sellecting Tools | Install Packages\u0026hellip; and typing the name of the desired package in the dialogue window.\nOnce installed, the package will apear in the list of the available packages in your Packages panel. To use it you have to load it to the system’s search path by simply typing the name of the package as an argument of the library() function, or by checking the box next to its name from the Packages panel.\n💡: Note that you can call the dialog window to install a package from Packages panel! Have you spotted the Install icon yet?\n Calculate in R To begin with, we can use R as a calculator. 😁\nIn your console type in 2 + 2. Note that you don’t have to type the equals sign and that answer has [1] in front. The [1] indicates that there is only one number in the answer. If the answer contains more than one number it uses numbering like this to indicate where in the ‘group’ of numbers each one is.\nYou see?! R is like a big calculator! 😲\nArithmetic and Logical Operators R’s binary and logical operators will look very familiar to those who have some experience with programming.\nArithmetic Operators Logical Operators Try doing other math operations, like subtraction, multiplication, division, or square root and power operations. For example, try the following:\n2 + 5 3 - 2 18 / 6 4 * 7 (5 - 3)^2 / 4 9^(1/2) * 4  Reproducibility: Save your scripts The code you type and want to be executed can be saved in scripts and R Markdown files. Scripts ending with .R file extension and R Markdown files, which mixes both R code and Markdown code, end with .Rmd.\n🤓💡: The code that you write just for quick exploration can be written in the console. Code we want to reuse and show off later should be saved as a script.\n To create a new script go through the menu File | New File | R Script or through the green plus button on the top left.\nAny code we type in here can be run and executed in the console. Hitting the Run button at the top of the script window will run the line of code on which the cursor is sitting.\nTo run multiple lines of code, highlight them and click Run.\n💡: Get into a habit of saving your scripts after you create them. Try to save them before running your code in case you write code that makes R crash which sometimes happens. 😣 \u0026amp;@#$!?#%\n https://www.rstudio.com/products/rstudio/features/\nR Objects R provides a number of specialised data structures we will refer to as objects. To refer to an object we use a symbol. You can assign any object using the assignment operator \u0026lt;-, which is a composite made up from ‘less than’ and ‘minus’, with no space between them! Thus, we can create scalar constants, which we refer to as variables, and perform mathematical operations over them.\nx \u0026lt;- 5 y \u0026lt;- 6  You can use objects in calculation in exactly the same way as as you have already seen numbers being used earlier:\nx + y  and you can store the results of the calculation done with the objects in another object:\nz \u0026lt;- x * y z  🤓💡 BUT, remember!!! Operator \u0026lt;- is a composite made up from ‘less than’ and ‘minus’, with no space between them!!!!\n Try to type the following\nx\u0026lt; -5 y\u0026lt; -6  and see what happens.\nAfter you’ve created some objects in R you can get a list of them using ls() function:\nls()  RStudio provides very comfortable working environment and enables you to monitor your list of objects in the Environment panel window in the top right corner.\nBuilt-In Functions R is not like other conventional statistical packages like SAS, Minitab, SPSS, to name a few. It is more of a programming language designed for conducting data analyses. It comes with a vast number of ready-made blocks of code that will enable you to manipulate data, perform intricate mathematical calculations with data, carry out an array of statistical analysis ranging from simple to complex to extremely complex and it will facilitate the creation of fantastic graphs. These pre-made blocks of code are known as functions.\nR has all the standard mathematical functions that you might ever need: sin, cos, tan, asin, atan, log, log10, exp, abs, sqrt, factorial… To use them, all you need to do is to type the function and put the name of the object (argument) you would like to use the function for in brackets.\nYou should try a few 😃\nsqrt(144) log10(8) log10(100) log(100) exp(1) pi sin(pi/2) abs(-7) factorial(3) exp(x) log(y, 2)  You can use expression as argument of a function:\nz \u0026lt;- x * y trunc(x^2 + z / y) log((100 * x - y^2) / z)  You can have nested functions and you can use functions in creating new objects:\nround(exp(x), 2) p \u0026lt;- abs(floor(log((100*x - y^2) / exp(z)))) p  🤓💡 To obtain a description of a function you need to type a question mark, ?, in front of the name of the function. You might find this particularly useful when you start applying more complicated functions, as help will often provide you not only with the detailed description of the function’s input/output arguments, but practical illustrative examples on how the function can be used and applied. You should try to ask R for hlep for lm() function (can you tell what it is used for?)\n Vectors When analysing data you are more likely to be working with lots of numbers/variables. It would be much more convenient to keep all of those numbers/variables as an object. Variables can be of a different type: logical, integer, double, string are some examples. Variables with one or more values of the same type are vectors. Hence, a variable with a single value (known to us as a scalar) is a vector of length 1. We can assign to vectors in many different ways:\n generated by R using the colon symbol (:) as a sequence generated operator or by using built in function rep() for replicating the given number for a given number of times.\nx \u0026lt;- 1:10 x  ## [1] 1 2 3 4 5 6 7 8 9 10  x \u0026lt;- rep(1,10) x  ## [1] 1 1 1 1 1 1 1 1 1 1  generated by the user by using concatenation function c that allows you to enter one number at a the time\nx \u0026lt;- c(2, 6, 4, 2, 3, 7, 1, 5, 9, 8) x  ## [1] 2 6 4 2 3 7 1 5 9 8  created as a sequence of numbers. For example to generate a sequence of numbers from 1 to 10, with increments of 0.2 type\nseq(1,10,0.2)  ## [1] 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 ## [15] 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 ## [29] 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 9.2 ## [43] 9.4 9.6 9.8 10.0   💡: R can easily perform arithmetic with vectors as it does with scalars. Thus, just as we can use those operators over the scalars we can use them when dealing with the vectors and/or a combination of both.\n x \u0026lt;- rep(1,10) y \u0026lt;- 1:10 x  ## [1] 1 1 1 1 1 1 1 1 1 1  y  ## [1] 1 2 3 4 5 6 7 8 9 10  c(x, y)  ## [1] 1 1 1 1 1 1 1 1 1 1 1 2 3 4 5 6 7 8 9 10  x + y  ## [1] 2 3 4 5 6 7 8 9 10 11  x + 2 * y  ## [1] 3 5 7 9 11 13 15 17 19 21  x^2 / y  ## [1] 1.0000000 0.5000000 0.3333333 0.2500000 0.2000000 0.1666667 0.1428571 ## [8] 0.1250000 0.1111111 0.1000000  z \u0026lt;- (x+y)/2 z  ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5  z \u0026lt;- c(z, rep(1, 3), c(100, 200, 300))+1 z  ## [1] 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 2.0 ## [12] 2.0 2.0 101.0 201.0 301.0  p \u0026lt;- 2.5 z*p  ## [1] 5.00 6.25 7.50 8.75 10.00 11.25 12.50 13.75 15.00 16.25 ## [11] 5.00 5.00 5.00 252.50 502.50 752.50  Accessing Vector’s Elements To access a specific element of a vector you would use index inside a single square bracket [] operator. The following shows how to obtain a vector member. The vector index is 1-based, thus use the index position 4 to access the fourth element.\nx \u0026lt;- c(9, 3, 7, 2, 9, 2, 1, 5, 4, 6) x  ## [1] 9 3 7 2 9 2 1 5 4 6  x[4]  ## [1] 2  💡: In R you can evaluate functions over the entire vectors which helps to avoid looping.\n y \u0026lt;- c(4, 1, 0, 8, 1, x) max(y)  ## [1] 9  range(y)  ## [1] 0 9  mean(y)  ## [1] 4.133333  var(y)  ## [1] 9.409524  sort(y)  ## [1] 0 1 1 1 2 2 3 4 4 5 6 7 8 9 9  cumsum(y)  ## [1] 4 5 5 13 14 23 26 33 35 44 46 47 52 56 62  💡: Note that missing values in R are represented by the symbol NA (not available) or NaN (not a number) for undefined mathematical operations.\n Here, NA would be shown if an index is out-of-range.\nz \u0026lt;- c(5, 8, 2) z[10]  ## [1] NA  You can also obtain a desirable selection of the elements of a vector by specifying a query within the index brackets []:\ny \u0026lt;- c(9, 3, 7, 2, 9, 2, 1, 5, 4, 6) y[y \u0026gt; 5]  ## [1] 9 7 9 6  y[y \u0026gt; 10]  ## numeric(0)  y[y != 2]  ## [1] 9 3 7 9 1 5 4 6  Matrices When data is arranged in two dimensions rather than one we have matrices. In R function matrix() creates matrices:\nma1 \u0026lt;- matrix(c(1, 0, -20, 0, 1, -15, 1, -1, 0), nrow = 3, ncol = 3, byrow = TRUE) ma1  ## [,1] [,2] [,3] ## [1,] 1 0 -20 ## [2,] 0 1 -15 ## [3,] 1 -1 0  dim(ma1)  ## [1] 3 3  The individual numbers in a matrix are called the elements of the matrix. Each element is uniquely defined by its particular row number and column number. To determine the dimensions of a matrix use function dim().\nAn element at the i-th row, j-th column of a matrix can be accessed by indexing inside square bracket operator [i, j]. The entire i-th row or entire j-th column of a matrix can be extracted as shown in the code below.\nma1 \u0026lt;- matrix(c(4, 10, 130, 0, 8, -7, 9, 11, -2, 7, -5, 4), nrow = 3, ncol = 4) ma1  ## [,1] [,2] [,3] [,4] ## [1,] 4 0 9 7 ## [2,] 10 8 11 -5 ## [3,] 130 -7 -2 4  ma1[2, 3]  ## [1] 11  ma1[1, ]  ## [1] 4 0 9 7  ma1[ , 4]  ## [1] 7 -5 4  Standard scalar algebra, which deals with operations on single numbers, has a set of well established rules for handling manipulations involving addition, subtraction, multiplication and division. In a broadly similar fashion a set of rules has been developed to enable us to manipulate matrices. However, introducing those rules is beyond the scope of this bootcamp, but they are nicely covered on the www.statisticshowto.com website.\nYOUR TURN 👇 Practise by doing the following set of exercises:\n1) Create vectors called x1 and x2, where vector x1 consists of numbers: 1, 4, 7, 9, 11, 12, 13, 15 and 18 and vector x2 of numbers: 1, 1, 1, 2, 2, 2, 3, 3, 3.\n2) Subtract x2 from x1.\n3) Create a new vector called x3 by combining vectors x1 and x2.\n4) Calculate mean and variance of x3.\n5) Calculate medians for the three vectors.\n6) Create a matrix called m1 with the following elements: matrix(c(1, 5, 9, 2, 6, 10, 3, 7, 11, 4, 8, 12), nrow = 4, ncol = 3, byrow = TRUE)\n7) Use a subscript to find the 2-nd number in vector x1 and x2 and element in the 2-nd raw and 3-rd column in matrix m1.\n8) Add the 5-th number in vector x1 to the element in matrix m1 which is in 1-st raw and 1-st column.\n9) Calculate the mean of all numbers in x3 that are less than 13.\n© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/statsconcepts/",
	"title": "Basic Stats Concepts",
	"tags": [],
	"description": "",
	"content": " In this section you will be introduced to a set of concepts that enable data to be explored with the objective\n of summarising and understanding the main features of the variables contained within the data and to investigate the nature of any linkages between the variables that may exist.  The starting point is to understand what data is.\n What is the population? Why do we use samples?  Can you provide a formal definition about the population and the sample? 😁\nThe population is the set of all people/objects of interest in the study being undertaken.\nIn statistical terms the whole data set is called the population. This represents “Perfect Information”, however in practice it is often impossible to enumerate the whole population. The analyst therefore takes a sample drawn from the population and uses this information to make judgements (inferences) about the population.\nClearly if the results of any analysis are based on a sample drawn from the population, then if the sample is going to have any validity, then the sample should be chosen in a way that is fair and reflects the structure of the population.\nThe process of sampling to obtain a representative sample is a large area of statistical study. The simplest model of a representative sample is a \u0026ldquo;random sample\u0026rdquo;:\nA sample chosen in such a way that each item in the population has an equal chance of being included in the sample.\nAs soon as sample data is used, the information contained within the sample is “Imperfect” and depends on the particular sample chosen. The key problem is to use this sample data to draw valid conclusions about the population with the knowledge of and taking into account the \u0026lsquo;error due to sampling\u0026rsquo;.\nThe importance of working with representative samples should be seriously considered; a good way to appreciate this importance is to see the consequences of using unrepresentative samples. A book by Darrell Huff called How to Lie with Statistics, published by Penguin contains several anecdotes of unrepresentative samples and the consequences of treating them as representative.\nData Analysis Using Sample Data Usually the data will have been collected in response to some perceived problem, in the hope of being able to glean some pointers from this data that will be helpful in the analysis of the problem. Data is commonly presented to the data analyst in this way with a request to analyse the data.\nBefore attempting to analyse any data, the analyst should:\ni) Make sure that the problem under investigation is clearly understood, and that the objectives of the investigation have been clearly specified.\nii) Before any analysis is considered the analyst should make sure that the individual variables making up the data set are clearly understood.\nThe analyst must understand the data before attempting any analysis.\nIn the summary you should ask yourself:\ni) Do I understand the problem under investigation and are the objectives of the investigation clear? The only way to obtain this information is to ask questions, and keep asking questions until satisfactory answers have been obtained.\nii) Do I understand exactly what each variable is measuring/recording?\nDescribing Variables A starting point is to examine the characteristics of each individual variable in the data set.\nThe way to proceed depends upon the type of variable being examined.\nClassification of variable types\nThe variables can be one of two broad types:\n Attribute variables: is a variable that has its outcomes described in terms of its characteristics or attributes.  gender days in a week    A common way of handling attribute data is to give it a numerical code. Hence, we often referred to them as coded variables.\n Measured variables: is a variable that has its outcomes taken from a numerical scale; the resulting outcome is expressed in numerical terms.\n weight age   There are two types of measured variables, a measured variable that is measured on some continuous scale of measurement, e.g. a person’s height. This type of measured variable is called a continuous variable. The other type of measured variable is a discrete variable. This results from counting; for example \u0026lsquo;the number of passengers on a given flight\u0026rsquo;.\nThe Concept of Statistical Distribution The concept of statistical distribution is central to statistical analysis.\nThis concept relates to the population and conceptually assumes that we have perfect information, the exact composition of the population is known.\nThe ideas and concepts for examining population data provide a framework for the way of examining data obtained from a sample. The Data Analyst classifies the variables as either attribute or measured and examines the statistical distribution of the particular sample variable from the sample data.\nFor an attribute variable the number of occurrences of each attribute is obtained, and for a measured variable the sample descriptive statistics describing the centre, width and symmetry of the distribution are calculated.\nattribute: measured: What does the distribution show? For an attribute variable it is very simple. We observe the frequency of occurance of each level of the attribute variable as shown in the barplot above.\nFor a measured variable the area under the curve from one value to another measures the relative proportion of the population having the oucome value in that range.\nA statistical distribution for a measured variable can be summarised using three key descriptions:\n the centre of the distribution; the width of the distribution; the symmetry of the distribution  The common measures of the centre of a distribution are the Mean (artithmetic average) and the Median. The median value of the variable is defined to be the particular value of the variable such that half the data values are less than the median value and half are greater\nThe common measures of the width of a distribution are the Standard Deviation and the Inter-Quartile Range. The Standard Deviation is the square root of the average squared deviation from the mean. Ultimately the standard deviation is a relative measure of spread (width), the larger the standard deviation the wider the distribution. The inter-quartile range is the range over which the middle 50% of the data values varies.\nBy analogy with the median it is possible to define the quartiles:\n Q1 is the value of the variable that divides the distribution 25% to the left and 75% to the right. Q2 is the value of the variable that divides the distribution 50% to the left and 50% to the right. This is the median by definition. Q3 is the value of the variable that divides the distribution 75% to the left and 25% to the right. The inter-quartile range is the value Q3 - Q1.  The diagram below shows this pictorially:\n🤓💡 Conventionally the mean and standard deviation are given together as one pair of measures of location and spread, and the median and inter-quartile range as another pair of measures.\nThere are a number of measures of symmetry; the simplest way to measure symmetry is to compare the mean and the median. For a perfectly symmetric distribution the mean and the median will be exactly the same. This idea leads to the definition of Pearson\u0026rsquo;s coefficient of Skewness as:\nPearson's coefficient of Skewness = 3(mean - median) / standard deviation\nAn alternative measure of Skewness is the Quartile Measure of Skewness defined as:\nQuartile Measure of Skewness = [(Q1 - Q3) - (Q2 - Q1)]/(Q3 - Q1)\nImportant Key Points:  What is Data? Variables Two types of variable:\n an attribute variable a measured variable  The concept of a Statistical Distribution:\n As applied to an attribute variable As applied to a measured variable  Descriptive Statistics for a measured variable:\n Measures of Centre  Mean, Median  Measures of Width:  Standard Deviation Inter-Quartile Range    The descriptive statistics provide a numerical description of the key parameters of the distribution of a measured sample variable.\nInvestigating relationship between variables One of the key steps required of the Data Analyst is to investigate the relationship between variables. This requires a further classification of the variables contained within the data, as either a response variable or an explanatory variable.\nA response variable is a variable that measures either directly or indirectly the objectives of the analysis.\nAn explanatory variable is a variable that may influence the response variable.\nBivariate Relationships In general there are four different combinations of type of Response Variable and type of Explanatory Variable, these four combinations are shown below:\nThe starting point for any investigation of the connections between a response variable and an explanatory variable starts with examining the variables, and defining the response variable, or response variables, and the explanatory variables.\n🤓💡: In large empirical investigations there may be a number of objectives and a number of response variables.\nThe method for investigating the connection between a response variable and an explanatory variable depends on the type of variables. The methodology is different for combination as shown in the box above, and applying an inappropriate method causes problems. 💡⚡️😩\nDA Methodology The first step is to have a clear idea of what is meant by a connection between the response variable and the explanatory variable. This will provide a framework for defining a Data-Analysis process to explore the connection between the two variables, and will utilise the ideas previously developed.\nThe next step is to use some simple sample descriptive statistics to have a first look at the nature of the link between the variables. This simple approach may allow the analyst to conclude that on the basis of the sample information there is strong evidence to support a link, or there is no evidence of a link, or that the simple approach is inconclusive and further more sophisticated data analysis is required. This step is called the Initial Data Analysis and sometimes abbreviated to IDA.\nIf the Initial Data Analysis suggests that Further Data Analysis (FDA) is required, then this step seeks one of two conclusions:\ni) The sample evidence is consistent with there being no link between the response variable and the explanatory variable. or ii) The sample evidence is consistent with there being a link between the response variable and the explanatory variable.\nThe outcome of the analysis is one of the two alternatives given above. If the outcome is that there is no evidence of a connection, then no further action is required by the analyst since the analysis is now complete.\nIf however the outcome of the analysis is that there is evidence of a connection, then the nature of the connection between the two variables needs to be described.\n🤓💡 The Data-Analysis Methodology described above seeks to find the answer to the following key question:\nOn the basis of the sample data is there evidence of a connection between the response variable and the explanatory variable?\nThe outcome is one of two conclusions\ni. No evidence of a relationship ii Yes there is evidence of a relationship, in which case the link needs to be described. This process can be represented diagrammatically as:\nFor each of the four data analysis situations given, the data analyst needs to know what constitutes the Initial Data Analysis (I.D.A.) and how to undertake and interpret the I.D.A. If Further Data Analysis is required the analyst needs to know how to undertake and interpret the Further Data Analysis.\nMeasured Vs Attribute(2-levels) There is a relationship between a measured response and an attribute explanatory variable if the average value of the response is dependent on the level of the attribute explanatory variable.\nGiven a measured response and an attribute explanatory variable with two levels, \u0026ldquo;red\u0026rdquo; \u0026amp; \u0026ldquo;blue\u0026rdquo;. If the statistical distribution of the response variable for attribute level \u0026ldquo;red\u0026rdquo; and attribute level \u0026ldquo;blue\u0026rdquo; are exactly the same then the level of the attribute variable has no influence on the value response, there is no relationship.\nThis can be illustrated as below:\nMeasured Vs Measured The first step is to have a clear idea of what is meant by a connection between a measured response variable and a measured explanatory variable. Imagine a population under study consisting of a very large number of population members, and on each population member two measurements are made, the value of Y the response variable and the value of X the explanatory variable. For the whole population a graph of Y against X could be plotted conceptually.\nIf the graph shows a perfect line, then there is quite clearly a link between Y and X. If the value of X is known, the exact value of Y can be read off the graph. This is an unlikely scenario in the data-analysis context, because this kind of relationship is a deterministic relationship. Deterministic means that if the value of X is known then the value of Y can be precisely determined from the relationship between Y and X. What is more likely to happen is that other variables may also have an influence on Y.\nIf the nature of the link between Y and X is under investigation then this could be represented as:\nY = f(X) + effect of all other variables [effect of all other variables is commonly abbreviated to e]\nConsidered the model:\nY = f(X) + e [e is the effect of all other variables]\nThe influence on the response variable Y can be thought of as being made up of two components:\ni) the component of Y that is explained by changes in the value of X, [the part due to changes in X through f(X)]\nii) the component of Y that is explained by changes in the other factors. [the part not explained by changes in X]\nOr in more abbreviated forms the \u0026lsquo;Variation in Y Explained by changes X\u0026rsquo; or \u0026lsquo;Explained Variation\u0026rsquo; and the \u0026lsquo;Variation in Y not explained by changes in X\u0026rsquo; or the \u0026lsquo;Unexplained Variation\u0026rsquo;.\nIn conclusion, the Total Variation in Y is made up of the two components:\n the \u0026lsquo;Changes in Y Explained by changes in X\u0026rsquo; and\n the \u0026lsquo;Changes in Y not explained by changes in X\u0026rsquo;  Which may be written as:\n`'The Total Variation in Y' = 'Explained Variation' + 'Unexplained Variation'`  🤓💡 The discussion started with the following idea:\nY = f(X) + e\nAnd to quantify the strength of the link the influence on Y was broken down into two components:\n'The Total Variation in Y' = 'Explained Variation' + 'Unexplained Variation'  This presents two issues: A: Can a model of the link be made? B. Can \u0026lsquo;The Total Variation in Y\u0026rsquo;, \u0026lsquo;Explained Variation\u0026rsquo; and the \u0026lsquo;Unexplained Variation\u0026rsquo; be measured?\nWhat do these quantities tell us?\nMaybe we can observe a propotion of the \u0026lsquo;Explained Variation in Y\u0026rsquo; over the \u0026lsquo;Total Variation in Y\u0026rsquo;. This ratio is always on the scale 0 to 1, but by convention is usually expressed as a percentage so is regarded as on the scale 0 to 100%. It is called \u0026lsquo;R Squared\u0026rsquo; and interpretation of this ratio is as follows:\nR_sq: 0% (no link) \u0026lt;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; 50% (Statistical Link) \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026gt; 100% (Perfect Link)\nThe definition and interpretation of R_sq is a very important tool in the data analyst tool kit for tracking connections between a measures response variable and a measured explanatory variable.\nWe can put those ideas into our DA Methodology frameworks as shown below.\n🤓💡 Note that you will hardly ever be in the situation in which R_sq would be so close to zero that would make you conclude that on the base of the sample evidence used in IDA it is possible to conclude that there is no relationship between the two variables. If R_sq value is very small (for example around 2%) this would need to be further tested to conclude if it is statistically significant based on the sample evidence by applying FDA.\nFurther Data Analysis If the \u0026lsquo;Initial Data Analysis\u0026lsquo; is inconclusive then \u0026lsquo;Further Data Analysis\u0026lsquo; is required.\nThe \u0026lsquo;Further Data Analysis\u0026rsquo; is procedure that enables a decision to be made, based on the sample evidence, as to one of two outcomes:\n- There is no relationship - There is a relationship\nThese statistical procedures are called hypothesis tests, which essentially provide a decision rule for choosing between one of the two outcomes: \u0026ldquo;There is no relationship\u0026rdquo; or \u0026ldquo;There is a relationship\u0026rdquo; based on the sample evidence.\nAll hypothesis tests are carried out in four stages: - Stage 1: S pecifying the hypotheses.\n Stage 2: Defining the test parameters and the decision rule.\n Stage 3: Examining the sample evidence.\n Stage 4: The conclusions.\n  Statistical Models used in FDA  Measured Response v Attribute Explanatory Variable with exactly two levels:\n t-test  Measured Response v Attribute Explanatory Variable with more than two levels:\n One-Way ANOVA   Measures Response v Measured Explanatory Variable\n Simple Regression Model   Measures Response v Measured Explanatory Variables\n Multifactor Regression Model  Attribute Response v Attribute Explanatory Variable\n Chi-Square Test of Independence   YOUR TURN 👇 Make sure you can answer the following questions:\n1) What are the underlying ideas that enable a relationship between two variables to be investigated?\n2) What is the purpose of summary statistics?\n3) What is the data analysis methodology for exploring the relationship between:\ni) a measured response variable and an attribute explanatory variable?\ni) a measured response variable and a measured explanatory variable?\n© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/datatypes/",
	"title": "Data Types",
	"tags": [],
	"description": "",
	"content": " The examples we have used in \u0026lsquo;How to Use R\u0026rsquo; section are all dealing with numbers (quantitative numerical data). Those of you familiar with programming will know that numerical objects can be classified as real, integer, double or complex. To check if an object is numeric and what type it is, you can use mode() and class() functions respectively.\nLet us go back into our R project and type the following into the open script file and run the code.😃\nx \u0026lt;- 1:10 mode(x)  ## [1] \u0026quot;numeric\u0026quot;  class(x)  ## [1] \u0026quot;integer\u0026quot;  In R to enter strings of characters as objects you need to enter them using quote marks around them. By default it expects all inputs to be numeric and unless you use quote marks around the strings you wish to enter, it will consider them as numbers and subsequently will return an error message.\nx \u0026lt;- c(\u0026quot;UK\u0026quot;, \u0026quot;Spain\u0026quot;, \u0026quot;Serbia\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;Italy\u0026quot;) mode(x)  ## [1] \u0026quot;character\u0026quot;  It is common in statistical data to have attributes also known as categorical variables. In R such variables should be specified as factors. Attribute variable has a set of levels indicating possible outcomes. Hence, to deal with x as an attribute variable with five levels we need to make it a factor in R.\nx \u0026lt;- c(\u0026quot;UK\u0026quot;, \u0026quot;Spain\u0026quot;, \u0026quot;Serbia\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;Italy\u0026quot;) x \u0026lt;- factor(x) x  ## [1] UK Spain Serbia France Germany Italy ## Levels: France Germany Italy Serbia Spain UK  💡: Note that R codes the factor levels in their alphabetical order. However, attribute variables are usual coded and you would usually enter them as such.\n quality \u0026lt;- factor(c(3, 3, 4, 2, 2, 4, 0, 1)) levels(quality)  ## [1] \u0026quot;0\u0026quot; \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot;  quality  ## [1] 3 3 4 2 2 4 0 1 ## Levels: 0 1 2 3 4  You might need to deal from time to time with logical data type. This is when something is recorded as TRUE or FALSE. It is most likely that you would use this data type when checking what type of data the variable is that you are dealing with. For example\nx \u0026lt;- c(\u0026quot;UK\u0026quot;, \u0026quot;Spain\u0026quot;, \u0026quot;Serbia\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;Italy\u0026quot;) is.numeric(x)  ## [1] FALSE  is.factor(x)  ## [1] FALSE  Data Frames Statistical data usually consists of several vectors of equal length and of various types that resemble a table. Those vectors are interconnected across so that data in the same position comes from the same experimental unit, ie. observation. R uses data frame for storing this kind of data table and it is regarded as primary data structure.\nLet us consider a study of share prices of companies from three different business sectors. As part of the study a random sample (n=15) of companies was selected and the following data was collected:\nshare_price \u0026lt;- c(880, 862, 850, 840, 838, 799, 783, 777, 767, 746, 692, 689, 683, 661, 658) profit \u0026lt;- c(161.3, 170.5, 140.7, 115.7, 107.9, 135.2, 142.7, 114.9, 110.4, 98.9, 90.2, 80.6, 85.4, 91.7, 137.8) sector \u0026lt;- factor(c(3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, + 1, 1)) # 1:IT, 2:Finance, 3:Pharmaceutical # share_price  ## [1] 880 862 850 840 838 799 783 777 767 746 692 689 683 661 658  profit  ## [1] 161.3 170.5 140.7 115.7 107.9 135.2 142.7 114.9 110.4 98.9 90.2 ## [12] 80.6 85.4 91.7 137.8  sector  ## [1] 3 3 3 3 3 2 2 2 2 2 1 1 1 1 1 ## Levels: 1 2 3  Rather than keeping this data as a set of individual vectors in R, it would be better to keep whole data as a single object, i.e. data frame.\nshare.data \u0026lt;- data.frame(share_price, profit, sector) share.data  Individual vectors from the data frame can be accessed using $ symbol:\nshare.data$sector  Now, as we have mastered the basics let us learn how to access existing data from R.\n© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/importexport/",
	"title": "Import Data inot R",
	"tags": [],
	"description": "",
	"content": " It’s the first big hurdle to dealing with data in R.\nYou are most likelly to have looked at, organised and examined your data files in Excel.\nOpening data in R is fairly simply, but organising it for analysis might take some thought and consideration. You\u0026rsquo;ll guess that it is possible to use File | Import Dataset menu option in RStudio to import your data, however we’re going to do it the proper way with a command.\nThere are many packages that let R import all types of data, but with the restriction of time in this bootcamp we will focus on CSV files and you can try to explore the rest for yourself.\nComma separated files are the most common way to save spreadsheets that don’t require a licenced, usually not free program to open. Importing CSV is part of base R and you can use read.csv() function to do so.\nread.csv() Let us go to https://data.gov.rs/ Serbian open data portal. In particular, let us try to access \u0026ldquo;Подаци из Огласа јавних набавки са Портала јавних набавки\u0026rdquo; and import this data to R.\nWe are not going to download it onto our local computers, but rather we will import it to R directly from the web using the link.\ndf_csv \u0026lt;- read.csv(\u0026quot;http://portal.ujn.gov.rs/OpenD/Tekuci_Mesec.csv\u0026quot;, stringsAsFactors = FALSE) head(df_csv, 10)  ## SifraNabavke ## 1 2447521 ## 2 2453300 ## 3 2458195 ## 4 2479362 ## 5 2479757 ## 6 2479911 ## 7 2483293 ## 8 2488566 ## 9 2495880 ## 10 2495922 ## NazivDokumenta ## 1 Прехрамбени производи ## 2 Набавка добара - рачунарска опрема 10/2019 ## 3 Екскурзија ученика трећег, четвртог, шестог, седмог и осмог разреда и настава у природи ученика другог разреда ## 4 ЗАКУП АУТОДИЗАЛИЦЕ И АУТОПЛАТФОРМЕ, ПАРТИЈА 1 - ЗАКУП АУТОДИЗАЛИЦЕ M2019-18 ## 5 Ауто гуме ## 6 Ангажовање стручне куће за безбедност на раду и против пожарне заштите, ЈНМВ 41-2019 ## 7 Настава у природи у пролеће 2020.године ## 8 Набавка ужине за децу у подручним групама - радна 2019/20. год. - Поновни поступак ## 9 Екскурзија и настава у природи за ученике ОШ \u0026quot;Коста Абрашевић\u0026quot; ## 10 Екскурзија ученика ОШ '' Дубрава '' у школској 2019/20 години ## NazivNarucioca MaticniBroj ## 1 Установа Геронтолошки центар Младеновац 17413805 ## 2 Факултет инжењерских наука Универзитета у Крагујевцу 7151314 ## 3 ОШ \u0026quot;Доситеј Обрадовић\u0026quot; 7141106 ## 4 ЈКП Зеленило и гробља Смедерево 20782633 ## 5 ЈАВНО КОМУНАЛНО ПРЕДУЗЕЋЕ \u0026quot;СТАНДАРД\u0026quot; 7208324 ## 6 Здравствени центар Врање 7205805 ## 7 ОШ''Николај Велимировић''Шабац 7120451 ## 8 ПУ \u0026quot;Чаролија\u0026quot; Ћићевац 7291825 ## 9 ОШ \u0026quot;КОСТА АБРАШЕВИЋ\u0026quot; 7004010 ## 10 Основна школа '' Дубрава '' 6950841 ## PIB IdMesto IdOpstina IdVrstaPostupka ## 1 101477704 704458 70173 1 ## 2 101576499 718980 70645 1 ## 3 101521949 733083 70947 1 ## 4 107333843 740527 71099 7 ## 5 100630696 717657 70602 7 ## 6 100548147 711306 70432 7 ## 7 101896237 746606 71269 7 ## 8 101510485 744328 71196 7 ## 9 100169916 791083 70211 1 ## 10 100407439 717657 70602 7 ## PredmetNabavke ## 1 храна, намирнице ## 2 рачунарска опрема (хардверска и софтверска) ## 3 услуге копненог саобраћаја (осим услуга железничког саобраћаја ), укључујући и услуге превоза у оклопљеним возилима и курирске услуге (осим превоза поште) ## 4 техничка опрема за обављање делатности (уређаји, машине, апарати, механизација и сл.) ## 5 друга добра ## 6 друге услуге ## 7 услуге хотела и ресторана ## 8 храна, намирнице ## 9 друге услуге ## 10 услуге у областима рекреације, културе и спорта ## VrstaPredmeta ## 1 добра ## 2 добра ## 3 услуге ## 4 добра ## 5 добра ## 6 услуге ## 7 услуге ## 8 добра ## 9 услуге ## 10 услуге ## OpstiRecnikNabavki ## 1 Свеже живинско месо,Месни производи,Припремљена и конзервисана риба,Животињска или биљна уља и масти,Млеко,Млинарски производи, скроб и скробни производи,Разни прехрамбени производи ## 2 Рачунарска опрема и материјал ## 3 Услуге организације путовања ## 4 Изнајмљивање дизалица са оператером ## 5 Гуме за тешка и лака возила ## 6 Услуге у области здравства и безбедности ## 7 Услуге друмског превоза,Најам возила за превоз путника са возачем,Пратеће и помоћне услуге превоза; услуге превозних агенција,Услуге путничких агенција и тур-оператера и услуге помоћи туристима ## 8 Храна, пиће, дуван и сродни производи ## 9 Услуге организације путовања ## 10 Услуге организације путовања ## KontaktOsoba Telefon ## 1 Новаковић Ђорђе 0118230865 ## 2 Марија Петровић, службеник за јавне набавке 034/330-196 ## 3 Весна Димитријевић 012223371 ## 4 Милица Бера 064-8476886 ## 5 Tasić Emilija 019/731-112 ## 6 Весна Стојановић 017427832 ## 7 Весна Павловић 350-483 (015) ## 8 Zvezdan Babic 037/811-354 ## 9 ВЕРИЦА ПЕТРОВИЋ 8041792 ## 10 Татјана Алексић 019/739-606 ## DatumPoslednjeIzmene IdDelatnost IdOblikOrg IdOblikSvoj IdKategorija ## 1 2019-10-01 13:14:13 8730 85 5 6 ## 2 2019-10-01 09:03:04 8542 85 5 5 ## 3 2019-10-01 11:16:50 8520 85 5 5 ## 4 2019-10-01 12:39:21 9603 17 5 7 ## 5 2019-10-01 13:35:25 3600 17 5 7 ## 6 2019-10-02 08:33:48 8610 85 5 3 ## 7 2019-10-01 11:58:27 8520 85 5 5 ## 8 2019-10-01 15:10:02 8510 85 5 5 ## 9 2019-10-02 15:52:54 8520 85 5 5 ## 10 2019-10-01 07:44:09 8520 85 5 5 ## PozivZaPodnosenjePonuda PozivZaPodnosenjePrijava ## 1 1 0 ## 2 1 0 ## 3 1 0 ## 4 2 0 ## 5 1 0 ## 6 1 0 ## 7 0 0 ## 8 1 0 ## 9 1 0 ## 10 1 0 ## ObavestenjeOSistemuDinamickeNabavke PozivZaUcesceNaKonkursZaDizajn ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 0 0 ## 10 0 0 ## ObavestenjeOPriznavanjuKvalifikacije ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 0 ## ObavestenjeOPriznavanjuKvalifikacijeURestriktivnomPostupku ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 0 ## PodaciOObjaviIDodeliUgovora ObavestenjeOZakljucenomOkvirnomSporazumu ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 0 0 ## 10 0 0 ## ObavestenjeOZakljucenomUgovoru ObavestenjeORezultatuKonkursa ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 0 0 ## 10 0 0 ## ObavestenjeOObustaviPostupkaJavneNabavke ## 1 0 ## 2 1 ## 3 0 ## 4 1 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 0 ## PodaciOIzmeniUgovoraOJavnojNabavci KonkursnaDokumentacija ## 1 0 1 ## 2 0 1 ## 3 0 1 ## 4 0 1 ## 5 0 1 ## 6 0 1 ## 7 0 1 ## 8 0 1 ## 9 0 1 ## 10 0 1 ## ObavestenjeOProduzenjuRoka PregovarackiBezPonuda ## 1 0 0 ## 2 0 0 ## 3 1 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 0 0 ## 10 0 0 ## ObavestenjeOZastitiPrava MisljenjeUpraveJN ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 0 0 ## 10 0 0 ## OdlukaOPriznavanjuKvalifikacije OdlukaOIskljucenjuKandidata ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 0 0 ## 10 0 0 ## OdlukaODodeliUgovora OdlukaODodeliOkvirnogSporazuma ## 1 7 0 ## 2 0 0 ## 3 2 0 ## 4 0 0 ## 5 1 0 ## 6 1 0 ## 7 0 0 ## 8 1 0 ## 9 0 0 ## 10 0 0 ## ObavestenjeOPonistenjuPostupka OdlukaOObustaviPostupka ## 1 0 0 ## 2 0 1 ## 3 0 0 ## 4 0 1 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 0 0 ## 10 0 0 ## OdlukaONastavkuPostupka OdlukaKomisijeOZakljucenjuUgovora ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 0 0 ## 10 0 0 ## ObavestenjeOPriznavanjuKvalifikacijeOpste ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 0 ## Link ## 1 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2447521 ## 2 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2453300 ## 3 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2458195 ## 4 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2479362 ## 5 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2479757 ## 6 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2479911 ## 7 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2483293 ## 8 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2488566 ## 9 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2495880 ## 10 http://portal.ujn.gov.rs/Dokumenti/JavnaNabavka.aspx?idd=2495922  The other way is to download it and to save it to your working directory.\nLet us download \u0026ldquo;Регистар повлачћених произвођача електричне енергије\u0026rdquo; and save it on your computers.\nMake sure you save the file into your R-Project working directory before you ask R to execute the following\ndf_csv \u0026lt;- read.csv(\u0026quot;poslodavci.csv\u0026quot;, stringsAsFactors = FALSE)  What do you think about this dataset?\nreadr::read_csv() Just so you can see how easy it is to use other packages for importing data into R the code below illustrates the use of read_csv().\n## If you don't have readr installed yet, uncomment and run the line below #install.packages(\u0026quot;readr\u0026quot;) library(readr) df_csv \u0026lt;- read_csv(\u0026quot;poslodavci.csv\u0026quot;) df_csv  You can use R with appropriate packages to access other data formats. For example jsonlite package for json files or foreign package for SPSS data files. Try to look through the help of the packages you\u0026rsquo;ve been introduced to and discover other options.\nYOUR TURN 👇 1) Open source software such as R and Git are extensively used in data science. Can you research the tools that are available for data science, both open and closed source. Write a short brief about the pros and cons for both types of software.\n2) Investigate the importance of open source data and identify where open source data can readily be found on the Internet.\n3) Write an explanation of the benefits of open source models for policy makings by local authorities and governments in general.\n© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/principlesvisualisation/",
	"title": "Introduction to Visualization Principles",
	"tags": [],
	"description": "",
	"content": " Designed to enable: exploration, discovery, communication Graphical methods are commonly used for exploratory data analysis. Boxplots, scatterplot matrices, nonparametric smoothers and tree diagrams are just some of the mostly used graphical tools for data exploration. This part of the course will provide practical recommendation for choosing the best chart or graph type for creating good and clear graphics that would serve the purpose of communicating the key information\n to intended audience and the publication to help readers answer the questions your graphic is showing to understand it easy without getting bewildered\n  Graphical Forms::Encoding Good and clear graphics rely most of all on a reliable data. Thus, the first principle of an effective visualisation is that it represents reliable information. The type of the information that needs to be communicated and displayed will direct the choice of the most appropriate type of data encoding that would make relevant patterns become noticeable. It is therefore important to understand the problem you wish to communicate and the type of the data you need for its communication from the statistical perspective, ie. is it measured, categorical (ordinal or nominal), time (temporal dimension) and geographical location (spatial dimensions) in case of spatiotemporal data.\n\nVisual encoding of a data set depends on the number and characteristics of the available attributes ie. variables and on the analytical problem in question. Alberto Cairo on his blog The Functional Art provides an effective list of the graphic forms used to encode data depending on the function of the display. The figure shows ranking of the elementary perception task according on how well they can be perceived based on the ground breaking work of Cleveland and McGill published in the paper of JASA while working in the famous AT\u0026amp;T Bell Labs. The above figure illustrates the order in which graphical forms could be placed based on the accuracy of the conclusions which readers can draw about the given data from them. If, for example, the goal of the graphic is to facilitate precise comparisons, Alberto in his book The Functional Art provides an effective illustration of superiorities between possible choices of the graphical forms that could be used.\nThere is not a specific methodology that is develop for choosing the most appropriate ways of encoding data. You never know if a visual form will work until you give it a try. It mostly depends on what your attributes are that you using in order to revel that special something from the data. However, there are some useful guidelines made by a few authors of which I would recommend to check:\n The Data Visualisation Catalogue by Severino Ribecca Chart Chooser by Depict Data Studio and Visual Vocabulary by by the Financial Times Visual Journalism Team  as a starting bench mark when creating a graph.\nOften, the graphical display of the information created for answering a specific question will invite further exploration, which is why it is important to present them in a clear and truthful manner. We should not forget that the sole purpose of data analysis, thus visualisation is to inform and to improve the knowledge. So yes, we should consider very carefully aesthetic appeal and design of the graph we create that could effectively engage with the audience, but should do so by focusing above all on accuracy, depth and clarity of its conveying information.\nIdentify encoding Let us play the game ‘Identify encoding!’. Converse with each other and make a list of graphical forms and the type of encodings used in each of the following visualisation:\n1) Visualisation: DESI\n2) Visualisation: DESI Report 2019 - Human Capital\n3) Visualisation: DESI Report 2019 - Human Capital\n4) Click on Visualisation: Gapminder Bubble Chart\n\n5) Click on Visualisation: Gapminder World Population\n\n6) Click on Visualisation: Periodic Table\n\nClick on this Visualisation too: A Periodic Table of visualisation methods\n7) Click on Visualisation: Clinton Email Network\n\nThe Complexity: effective visual design The most important to remember when creating a graphical is to present data clearly and truthfully. The choice of the scale on the chart should show the differences in the data and communicates the range of values accurately. Any statistical summary displayed on the chart should be presented clearly with the source of information and statistics used to calculate the more complex figures.\nHere are some most obvious issues you need to pay attention to when designing an effective graph:\n1) Choose a scale for your charts that strikes a balance between demonstrating trends clearly and conveying the scale of the original dataset. The chart does not need to begin at 0 in order to establish a meaningful baseline if another logical starting point exists. Dual-Scaled Axes in Graphs\nThe choice of scale should allow for better accuracy that reader can draw about the information displayed on the chart.\nHave a look at What to consider when creating a line chart blog post by Lisa Charlotte Rost, where you will also find the links to some more interesting posts on this topic.\n 2) Emphasise what\u0026rsquo;s important: Identify what is the key information you are trying to communicate and think of the most effective format to do so as graphs can help you to express complex data in a simple format. Displaying an important item in a different colour is an easy way to draw attention to a point-making value.\nSometimes it might be effective to pull the key information from a chart into a separathe graphs and to present them in parralell.\nKeep in mind that taming the information from the visual display should not be difficult.\n 3) Declutter the chart – keep it simple as effective visualisations show the data to tell the story. Graphs would not look better by piling on the information and bombarding on with fancy ‘viz’ skills. Effective data visualisation is a delicate balancing act between form and function. Keep the focus on the important points by reducing unnecessay visual stimulus.\nIntegrate the text with the graphs only if necessary to help better convey information effigy.\nIn this presentation Alberto Cairo illustrates the importance of a good choice of the format used to visualy explain the main story: \u0026ldquo;How Music Preferences Have Changed in Two Decades\u0026rdquo;.\nWhen creating a data visualisation, think about the specific information that you want your data to convey, or the outcome that you want to achieve. Keep it simple and remove any unnecessary elements that could convolute your central point, bombarding an audience with too much data will likely leave them doubtful and confused.\n Going beyond obvious When creating a graphical display focus on best practices and explore your own personal style. Build a foundation of exploring and summurising a set of numbers and identifying the key feature within the data that would help you in presenting you visual data story.\nDual Axis Charts There is a belief that charts with two different y-axes make it hard for most people to read and to make right conclusions about two data sets. Having a secondary y-axis often creates a confusion as it is not clear which data to read against which axis. The main danger of dual axis charts is that they’re not intuitive. There are many people who are opposing them as they often create confusion and assum correlation when there is none. Stephen Few however, has wrote a well-argued paper in which he carefully presents issues one needs to take into consideration when wanting to use them. You need to judge for yourself if you are going to be fun or not of dual y-axis charts. It should certainly depend on your judgment of its suitability for conveying your graphical story telling as with any other graphical format.\nInteractive Charts Interactivity allows the viewer to engage with data in ways impossible by static graphs. One of the key benefits of interactive data visualizations is their flexibility in allowing further manipulation and exploration of the data used. By enabling concentrated focus by ‘zoom in’ facility it makes discovery of the seemingly small facts in a big story easy and engaging as users are invited to pose additional questions and come up to the new findings. Interactive graphical story telling is a rich and powerful tool for displaying data features as it enables viewers to dive into the data story as little or as much as they’re interested.\n\nGraphical Displays in R: from where to start? R is a great tool for data visualisation not just for people who want to develop understanding about their data using graphs, but nayone with need to produce high-quality and effective graphics to enhance their reports, web pages, or other documents.\nTake a look at The R Graph Gallery that provides an extensive collection of charts made in R together with the code.\nThere are several clasic books on drwing graphics in R, such as:\n ggplot2 [Wickham, 2009] Lattice [Sarkar, 2008] and R Graphics [Murrell, 2011].  If you would like to get more serious and systematic in your learning about graphical data analysis with R Graphical Data Analysis with R book by Antony Unwin is a great place to start. The R code for every graphic and analysis in the book is available from the book\u0026rsquo;s website.\nData Visualization with R by Rob Kabacoff is another good book about graphical displayes in R. It gives a systematic overview for creating graphical display in R starting with a brief introduction to R and following by a comprehensive list of graphical forms commanly used in statistical modelling, geospatial mapping and finishing with interactive plots.\nBack in 2012 there was a very engaging academic discussion between Andrew Gelman, Anthony Unwin, Stephen Few, Paul Murrell, Hadley Wickham and Robert Kosara about visualisation and infographics. Robert compiles all the discussion posts in this blog post that makes riveting reading.\nPractise! Gaining experience in interpreting graphics and drawing your own data displays is the most effective way of becaming a data viz.\n © 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/",
	"title": "data challenge with R",
	"tags": [],
	"description": "",
	"content": " data challenge with R Tatjana Kecojevic, SisterAnalyst  \u0026ldquo;The greatest value of a picture is when it forces us to notice what we never expected to see\u0026hellip; Let us take boring flat data and bring it to life through visualization.\u0026rdquo;\nJohn Tukey\n  Open data is data in digital format that is made available to everyone. To access it without the need for negotiation or obstruction at any time is a fundamental feature of the democratisation of digital data. In order for this to happen it requires public authorities to make data available to everyone, in particular those with the necessary skills and knowledge to reveal and interpret the story within complex details. A greater understanding of the data empowers citizens to make more informed decisions in processes that directly affect their lives.\n  R together with RStudio is the best data science tool! It is open source and free software that is available to anyone with a desire to discover, learn, explore, experience, expand and share the algorithms of their data science journey.\n To book your place go here 👉📩 In partnership with \n© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]