[
{
	"uri": "/day2/datawrangling/",
	"title": "Data Wrangling",
	"tags": [],
	"description": "",
	"content": " How do we do it? ü§î This diagram is taken from R for Data Science book by Garrett Grolemund and Hadley Wickham, which it is a great resource for learning R. There is a whole community built around it and you could join it and start learning together: R4DS online learning community.\nDataset To learn and practise how to organise a data we will use a gapminder data set available from gapminder package in R. This dataset is put into R by Jennifer Bryan from a tank of data sets available from Gapminder.\nGapminder is an independent Swedish foundation that helps to promote sustainable global development by collecting and analysiing relevant data and by developing and designing teaching/learning tools. Gapminder was founded in Sweden by Hans Rosling who was a mastermind for distinctive and insightful storytelling about global development using visual animation.\nYou can see Hans in action in this BBC documentary The joy of Stats available on YouTube.\nGapminder Data For each of 142 countries, the package provides values for life expectancy, GDP per capita, and population, every five years, from 1952 to 2007.\nBefore you can take a look at this data set first run the folowing code\n# install necessary packages: install.packages(\u0026quot;dplyr\u0026quot;, repos = \u0026quot;http://cran.us.r-project.org\u0026quot;) install.packages(\u0026quot;ggplot2\u0026quot;, repos = \u0026quot;http://cran.us.r-project.org\u0026quot;) install.packages(\u0026quot;gapminder\u0026quot;, repos = \u0026quot;http://cran.us.r-project.org\u0026quot;)  # have a look at the data head(gapminder::gapminder)  ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786.  üí° Note thet there are 6 columns, each of which we call a variable.\n Description: Excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country.\nThe main data frame gapminder has 1704 rows and 6 variables: - country: factor with 142 levels - continent: factor with 5 levels - year: ranges from 1952 to 2007 in increments of 5 years - lifeExp: life expectancy at birth, in years - pop: population - gdpPercap: GDP per capita\ngapminder::gapminder[1:3,]  ## # A tibble: 3 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853.  1st look at the data using the following functions: dim() \u0026amp; head()\nlibrary(gapminder) dim(gapminder)  ## [1] 1704 6  head(gapminder, n=10)  ## # A tibble: 10 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635.  Can you tell what each of the two functions does‚ÅâÔ∏è\nDo we have the information about the structure of the data? ü§î We can examine the structure using str() function, but the output could look messy and hard to follow if the data set is big. ü§™\nstr(gapminder)  ## Classes 'tbl_df', 'tbl' and 'data.frame':\t1704 obs. of 6 variables: ## $ country : Factor w/ 142 levels \u0026quot;Afghanistan\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ continent: Factor w/ 5 levels \u0026quot;Africa\u0026quot;,\u0026quot;Americas\u0026quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ year : int 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num 28.8 30.3 32 34 36.1 ... ## $ pop : int 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... ## $ gdpPercap: num 779 821 853 836 740 ...  The dplyr Package The dplyr provides a ‚Äúgrammar‚Äù (the verbs) for data manipulation and for operating on data frames in a tidy way. The key operator and the essential verbs are:\n%\u0026gt;%: the ‚Äúpipe‚Äù operator used to connect multiple verb actions together into a pipeline.\nselect(): return a subset of the columns of a data frame.\nmutate(): add new variables/columns or transform existing variables.\nfilter(): extract a subset of rows from a data frame based on logical conditions.\narrange(): reorder rows of a data frame according to single or multiple variables.\nsummarise() / summarize(): reduce each group to a single row by calculating aggregate measures.\nWe can have a look at the data and its structure by using the glimpse() function from the dplyr package.\nsuppressPackageStartupMessages(library(dplyr)) glimpse(gapminder)  ## Observations: 1,704 ## Variables: 6 ## $ country \u0026lt;fct\u0026gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Af‚Ä¶ ## $ continent \u0026lt;fct\u0026gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, ‚Ä¶ ## $ year \u0026lt;int\u0026gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, ‚Ä¶ ## $ lifeExp \u0026lt;dbl\u0026gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854‚Ä¶ ## $ pop \u0026lt;int\u0026gt; 8425333, 9240934, 10267083, 11537966, 13079460, 148803‚Ä¶ ## $ gdpPercap \u0026lt;dbl\u0026gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.‚Ä¶  ü§ìüí°: Notice how we could prevent display of the messages that appear when uploading the packages by using, in this case, suppressPackageStartupMessages()!\n Now we have the dplyr package uploaded, let us learn its verbs. üòá\nThe pipeline operater: %\u0026gt;% Left Hand Side (LHS) %\u0026gt;% Right Hand Side (RHS)\nx %\u0026gt;% f(\u0026hellip;, y)\nf(x,y)\nThe \u0026ldquo;pipe\u0026rdquo; passes the result of the *LHS as the 1st operator argument of the function on the RHS.\n 3 %% sum(4)  sum(3, 4)  %\u0026gt;% is very practical for chaining together multiple dplyr functions in a sequence of operations.\nPick variables by their names: select(),  starts_with(\u0026quot;X\u0026quot;) every name that starts with \u0026ldquo;X\u0026rdquo;.\n ends_with(\u0026quot;X\u0026quot;) every name that ends with \u0026ldquo;X\u0026rdquo;.\n contains(\u0026quot;X\u0026quot;) every name that contains \u0026ldquo;X\u0026rdquo;.\n matches(\u0026quot;X\u0026quot;) every name that matches \u0026ldquo;X\u0026rdquo;, where \u0026ldquo;X\u0026rdquo; can be a regular expression.\n num_range(\u0026quot;x\u0026quot;, 1:5) the variables named x01, x02, x03, x04, x05.\n one_of(x) =\u0026gt; every name that appears in x, which should be a character vector.\n  üëâ Practice ‚è∞üíª: Select your variables 1) that ends with letter p\n2) starts with letter co. Try to do this selection using base R.\nüòÉüôå Solutions: gm_pop_gdp \u0026lt;- select(gapminder, ends_with(\u0026quot;p\u0026quot;)) head(gm_pop_gdp, n = 1)  ## # A tibble: 1 x 3 ## lifeExp pop gdpPercap ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 28.8 8425333 779.  gm_cc \u0026lt;- select(gapminder, starts_with(\u0026quot;co\u0026quot;)) head(gm_cc, n = 1)  ## # A tibble: 1 x 2 ## country continent ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Afghanistan Asia  of course all of this could be done using base R for example:\ngm_cc \u0026lt;- gapminder[c(\u0026quot;country\u0026quot;, \u0026quot;continent\u0026quot;)]  but it\u0026rsquo;s less intuitive and often requires more typing.\nCreate new variables of existing variables: mutate() It would allow you to add to the data frame df a new column, z, which is the multiplication of the columns x and y: mutate(df, z = x * y). If we would like to observe lifeExp measured in months we could create a new column lifeExp_month:\ngapminder2 \u0026lt;- mutate(gapminder, LifeExp_month = lifeExp * 12) head(gapminder2, n = 2)  ## # A tibble: 2 x 7 ## country continent year lifeExp pop gdpPercap LifeExp_month ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 346. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 364.  Pick observations by their values: filter() There is a set of logical operators in R that you can use inside filter():\n x \u0026lt; y: TRUE if x is less than y x \u0026lt;= y: TRUE if x is less than or equal to y x == y: TRUE if x equals y x != y: TRUE if x does not equal y x \u0026gt;= y: TRUE if x is greater than or equal to y x \u0026gt; y: TRUE if x is greater than y x %in% c(a, b, c): TRUE if x is in the vector c(a, b, c) is.na(x): Is NA !is.na(x): Is not NA  üëâ Practice ‚è∞üíª: Filter your data Use gapminder2 df to filter:\n1) only European countries and save it as gapmEU\n2) only European countries from 2000 onward and save it as gapmEU21c\n3) rows where the life expectancy is greater than 80\nDon\u0026rsquo;t forget to use == instead of =! and Don\u0026rsquo;t forget the quotes \u0026quot;\u0026quot;\nüòÉüôå Solutions: gapmEU \u0026lt;- filter(gapminder2, continent == \u0026quot;Europe\u0026quot;) head(gapmEU, 2)  ## # A tibble: 2 x 7 ## country continent year lifeExp pop gdpPercap LifeExp_month ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Albania Europe 1952 55.2 1282697 1601. 663. ## 2 Albania Europe 1957 59.3 1476505 1942. 711.  gapmEU21c \u0026lt;- filter(gapminder2, continent == \u0026quot;Europe\u0026quot; \u0026amp; year \u0026gt;= 2000) head(gapmEU21c, 2)  ## # A tibble: 2 x 7 ## country continent year lifeExp pop gdpPercap LifeExp_month ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Albania Europe 2002 75.7 3508512 4604. 908. ## 2 Albania Europe 2007 76.4 3600523 5937. 917.  filter(gapminder2, lifeExp \u0026gt; 80)  Reorder the rows: arrange() is used to reorder rows of a data frame (df) according to one of the variables/columns.\n If you pass arrange() a character variable, R will rearrange the rows in alphabetical order according to values of the variable.\n If you pass a factor variable, R will rearrange the rows according to the order of the levels in your factor (running levels() on the variable reveals this order).\n  üëâ Practice ‚è∞üíª: Arranging your data 1) Arrange countries in gapmEU21c df by life expectancy in ascending and descending order.\n2) Using gapminder df - Find the records with the smallest population\n Find the records with the largest life expectancy.  üòÉüôå Solution 1): gapmEU21c_h2l \u0026lt;- arrange(gapmEU21c, lifeExp) head(gapmEU21c_h2l, 2)  ## # A tibble: 2 x 7 ## country continent year lifeExp pop gdpPercap LifeExp_month ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Turkey Europe 2002 70.8 67308928 6508. 850. ## 2 Romania Europe 2002 71.3 22404337 7885. 856.  gapmEU21c_l2h \u0026lt;- arrange(gapmEU21c, desc(lifeExp)) # note the use of desc() head(gapmEU21c_l2h, 2)  ## # A tibble: 2 x 7 ## country continent year lifeExp pop gdpPercap LifeExp_month ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Iceland Europe 2007 81.8 301931 36181. 981. ## 2 Switzerland Europe 2007 81.7 7554661 37506. 980.  üòÉüôå Solution 2): arrange(gapminder, pop)  ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Sao Tome and Principe Africa 1952 46.5 60011 880. ## 2 Sao Tome and Principe Africa 1957 48.9 61325 861. ## 3 Djibouti Africa 1952 34.8 63149 2670. ## 4 Sao Tome and Principe Africa 1962 51.9 65345 1072. ## 5 Sao Tome and Principe Africa 1967 54.4 70787 1385. ## 6 Djibouti Africa 1957 37.3 71851 2865. ## 7 Sao Tome and Principe Africa 1972 56.5 76595 1533. ## 8 Sao Tome and Principe Africa 1977 58.6 86796 1738. ## 9 Djibouti Africa 1962 39.7 89898 3021. ## 10 Sao Tome and Principe Africa 1982 60.4 98593 1890. ## # ‚Ä¶ with 1,694 more rows  arrange(gapminder, desc(lifeExp))  ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Japan Asia 2007 82.6 127467972 31656. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. ## 3 Japan Asia 2002 82 127065841 28605. ## 4 Iceland Europe 2007 81.8 301931 36181. ## 5 Switzerland Europe 2007 81.7 7554661 37506. ## 6 Hong Kong, China Asia 2002 81.5 6762476 30209. ## 7 Australia Oceania 2007 81.2 20434176 34435. ## 8 Spain Europe 2007 80.9 40448191 28821. ## 9 Sweden Europe 2007 80.9 9031088 33860. ## 10 Israel Asia 2007 80.7 6426679 25523. ## # ‚Ä¶ with 1,694 more rows  Collapse many values down to a single summary: summarise() The syntax of summarise() is simple and consistent with the other verbs included in the dplyr package.\n uses the same syntax as mutate(), but the resulting dataset consists of a single row instead of an entire new column in the case of mutate().\n builds a new dataset that contains only the summarising statistics.\n     Objective Function Description     basic sum(x) Sum of vector x   centre mean(x) Mean (average) of vector x    median(x) Median of vector x   spread sd(x) Standard deviation of vector x    quantile(x, probs) Quantile of vector x    range(x) Range of vector x    diff(range(x))) Width of the range of vector x    min(x) Min of vector x    max(x) Max of vector x    abs(x) Absolute value of a number x    üëâ Practice ‚è∞üíª: Use summarise(): 1) to print out a summary of gapminder containing two variables: max_lifeExp and max_gdpPercap.\n2) to print out a summary of gapminder containing two variables: mean_lifeExp and mean_gdpPercap.\nüòÉüôå Solution: Summarise your data summarise(gapminder, max_lifeExp = max(lifeExp), max_gdpPercap = max(gdpPercap))  ## # A tibble: 1 x 2 ## max_lifeExp max_gdpPercap ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 82.6 113523.  summarise(gapminder, mean_lifeExp = mean(lifeExp), mean_gdpPercap = mean(gdpPercap))  ## # A tibble: 1 x 2 ## mean_lifeExp mean_gdpPercap ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 59.5 7215.  Subsetting: group_by() dplyr\u0026rsquo;s group_by() function enables you to group your data. It allows you to create a separate df that splits the original df by a variable.\nThe function summarise() can be combined with group_by().\n   Objective Function Description     Position first() First observation of the group    last() Last observation of the group    nth() n-th observation of the group   Count n() Count the number of rows    n_distinct() Count the number of distinct observations    üëâ Practice ‚è∞üíª: Subset your data 1) Identify how many countries are given in gapminder data for each continent.\nüòÉüôå Solution: gapminder %\u0026gt;% group_by(continent) %\u0026gt;% summarise(n_distinct(country))  ## # A tibble: 5 x 2 ## continent `n_distinct(country)` ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Africa 52 ## 2 Americas 25 ## 3 Asia 33 ## 4 Europe 30 ## 5 Oceania 2  Let\u0026rsquo;s %\u0026gt;% all up! You can try to get into a habit of using a shortcut for the pipe operator üó£üë• Confer with your neighbours: What relationship do you expect to see between population size (pop) and life expectancy (lifeExp)?\nLook what this code produces\ngapminder_pipe \u0026lt;- gapminder %\u0026gt;% filter(continent == \u0026quot;Europe\u0026quot; \u0026amp; year == 2007) %\u0026gt;% mutate(pop_e6 = pop / 1000000) plot(gapminder_pipe$pop_e6, gapminder_pipe$lifeExp, cex = 0.5, col = \u0026quot;red\u0026quot;)  tidyr The tidyr can help you to create tidy data. Tidy data is data where:\n Every column is a variable Every row is an observation Every cell is a single value  tidyr package embraces the principles of tidy data and provides the standard key functionalities to organise data values within a dataset.\nHadley Wickham the author of the tidyr package talks in his paper Tidy Data about the importance of data cleaning process and structuring datasets to facilitate data analysis.\nReal datasets, you most likely are going to download from https://data.gov.rs/ or any other open source data platform, would often violate the three precepts of tidy data in all kind of different ways:\n Variables would not have their names and column headers are values. A number of variables are stored in one column A single variable that is stored in several columns Same information stored multiple times as different variables\u0026hellip;  to name a few.\nTo illustrate this, let us go back onto https://data.gov.rs/ and access Kvalitet Vazduha 2017. In particular, we want to access 2017-NO2.csv data.\n## If you don't have tidyr installed yet, uncomment and run the line below #install.packeges(\u0026quot;tidyr\u0026quot;) library(tidyr) # access 2017-NO2.csv data no2 \u0026lt;- read.csv(\u0026quot;http://data.sepa.gov.rs/dataset/ca463c44-fbfa-4de9-9a75-790995bf2830/resource/74516688-5fb5-47b2-becc-6b6e31a24d80/download/2017-no2.csv\u0026quot;, stringsAsFactors = FALSE, fileEncoding = \u0026quot;latin1\u0026quot;) # have a look at the data glimpse(no2)  ## Observations: 365 ## Variables: 8 ## $ Datum \u0026lt;chr\u0026gt; \u0026quot;01.01.2017\u0026quot;, \u0026quot;02.01.2017\u0026quot;, \u0026quot;03.01.2017\u0026quot;‚Ä¶ ## $ Novi.Sad.SPENS.NO2 \u0026lt;dbl\u0026gt; 22.89, 32.94, 14.86, 22.73, 20.89, 10.47‚Ä¶ ## $ Beograd.Mostar.NO2 \u0026lt;dbl\u0026gt; 28.83, 39.12, 25.20, 28.48, 27.82, 41.91‚Ä¶ ## $ Beograd.Vra√®ar.NO2 \u0026lt;dbl\u0026gt; 182.84, 244.56, 147.49, 159.12, 124.16, ‚Ä¶ ## $ Beograd.Zeleno.brdo.NO2 \u0026lt;dbl\u0026gt; 36.32, 36.68, 27.00, 35.15, 24.92, 8.27,‚Ä¶ ## $ Kragujevac..NO2 \u0026lt;dbl\u0026gt; 38.92, 43.50, 40.17, 45.44, 43.04, 34.55‚Ä¶ ## $ U.ice..NO2 \u0026lt;dbl\u0026gt; 49.84, 50.34, 36.35, 49.07, 33.33, 39.88‚Ä¶ ## $ Ni..IZJZ.Ni...NO2 \u0026lt;dbl\u0026gt; 18.61, 22.87, 15.68, 21.97, 21.68, 12.11‚Ä¶  It shows that this data set has 365 observations and 8 variables. Nonetheless, we need to consider what type of information do we have here:\n date of NO2 measurment taken: given in a single column -\u0026gt; ‚úÖ tidyüôÇ places where the measure of NO2 was taken: given in several columns -\u0026gt; ‚ùé tidyüôÅ NO2 measurements: given in several columns -\u0026gt; ‚ùé tidyüôÅ  Hmmm‚Ä¶ ü§î This doesn‚Äôt look tidy at all üò≥\nThis data is about measurement level of NO2(¬µg/m3) in several different towns/places, which means that NO2 is our main response variable. The way in which this variable is given in this data is certainly not tidy. It defeats the key principles of tidy data: Every column is a variable and furthermore, Every row is NOT an observation.\nIt appears that this data has 8 variables, but we have realised that there are only 3: date, place and no2. To tidy it, we need to stack it by turning columns into rows. We are happy with the variable date and it should remain as a single column, the other 7 columns we want to convert into two variables: place and no2.\nTo make wide format data into tall format we have toturn columns into rows using gather() function.\nWe will create variable place in which we will hold the headers as given in the columns 2:8. The values inside those columns will be saved in the new variable no2.\nnew_no2 \u0026lt;- no2 %\u0026gt;% gather(\u0026quot;place\u0026quot;, \u0026quot;no2\u0026quot;, -Datum, factor_key = TRUE) # stack all columns apart from `Datum` glimpse(new_no2)  ## Observations: 2,555 ## Variables: 3 ## $ Datum \u0026lt;chr\u0026gt; \u0026quot;01.01.2017\u0026quot;, \u0026quot;02.01.2017\u0026quot;, \u0026quot;03.01.2017\u0026quot;, \u0026quot;04.01.2017\u0026quot;, \u0026quot;0‚Ä¶ ## $ place \u0026lt;fct\u0026gt; Novi.Sad.SPENS.NO2, Novi.Sad.SPENS.NO2, Novi.Sad.SPENS.NO2‚Ä¶ ## $ no2 \u0026lt;dbl\u0026gt; 22.89, 32.94, 14.86, 22.73, 20.89, 10.47, 9.58, 15.99, 14.‚Ä¶  Let us see the names of the places\nnew_no2 %\u0026gt;% group_by(place) %\u0026gt;% summarise(n())  ## # A tibble: 7 x 2 ## place `n()` ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Novi.Sad.SPENS.NO2 365 ## 2 Beograd.Mostar.NO2 365 ## 3 Beograd.Vra√®ar.NO2 365 ## 4 Beograd.Zeleno.brdo.NO2 365 ## 5 Kragujevac..NO2 365 ## 6 U.ice..NO2 365 ## 7 Ni..IZJZ.Ni...NO2 365  Those names look very messy. We could use function from stringr package str_sub(). To begin with let\u0026rsquo;s get rid off .NO2 at the end of each name.\n## If you don't have stringr installed yet, uncomment and run the line below #install.packeges(\u0026quot;stringr\u0026quot;) library(stringr) new_no2$place \u0026lt;- new_no2$place %\u0026gt;% str_sub(end = -5) glimpse(new_no2)  ## Observations: 2,555 ## Variables: 3 ## $ Datum \u0026lt;chr\u0026gt; \u0026quot;01.01.2017\u0026quot;, \u0026quot;02.01.2017\u0026quot;, \u0026quot;03.01.2017\u0026quot;, \u0026quot;04.01.2017\u0026quot;, \u0026quot;0‚Ä¶ ## $ place \u0026lt;chr\u0026gt; \u0026quot;Novi.Sad.SPENS\u0026quot;, \u0026quot;Novi.Sad.SPENS\u0026quot;, \u0026quot;Novi.Sad.SPENS\u0026quot;, \u0026quot;Nov‚Ä¶ ## $ no2 \u0026lt;dbl\u0026gt; 22.89, 32.94, 14.86, 22.73, 20.89, 10.47, 9.58, 15.99, 14.‚Ä¶  new_no2 %\u0026gt;% group_by(place) %\u0026gt;% summarise(n())  ## # A tibble: 7 x 2 ## place `n()` ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Beograd.Mostar 365 ## 2 Beograd.Vra√®ar 365 ## 3 Beograd.Zeleno.brdo 365 ## 4 Kragujevac. 365 ## 5 Ni..IZJZ.Ni.. 365 ## 6 Novi.Sad.SPENS 365 ## 7 U.ice. 365  It still doesn\u0026rsquo;t look right. üòü This could be a tedious job. üò• It is no wonder why many data analysts grumble about time spent on the process of cleaning and preparing the data. It could be a very long and time consuming process, but the more you do it and more experience you gain the easier and less painful it gets.\nPerhaps, you can try to explore other available packages in R that could help you with organising your data into your ideal format. To give you an idea we will show you how it could be easily done when using forcats::fct_recode() function.\n## If you don't have forcats installed yet, uncomment and run the line below #install.packages(\u0026quot;forcats\u0026quot;) library(forcats) new_no2 \u0026lt;- no2 %\u0026gt;% gather(\u0026quot;place\u0026quot;, \u0026quot;no2\u0026quot;, -Datum, factor_key = TRUE) %\u0026gt;% # stack all columns apart from `Datum` mutate(place = fct_recode(place, \u0026quot;NS_Spens\u0026quot; = \u0026quot;Novi.Sad.SPENS.NO2\u0026quot;, \u0026quot;BG_Most\u0026quot; = \u0026quot;Beograd.Mostar.NO2\u0026quot;, \u0026quot;BG_Vracar\u0026quot; = \u0026quot;Beograd.Vra√®ar.NO2\u0026quot;, \u0026quot;BG_ZelenoBrdo\u0026quot; = \u0026quot;Beograd.Zeleno.brdo.NO2\u0026quot;, \u0026quot;KG\u0026quot; = \u0026quot;Kragujevac..NO2\u0026quot;, \u0026quot;NI\u0026quot; = \u0026quot;Ni..IZJZ.Ni...NO2\u0026quot;, \u0026quot;UZ\u0026quot; = \u0026quot;U.ice..NO2\u0026quot;)) glimpse(new_no2)  ## Observations: 2,555 ## Variables: 3 ## $ Datum \u0026lt;chr\u0026gt; \u0026quot;01.01.2017\u0026quot;, \u0026quot;02.01.2017\u0026quot;, \u0026quot;03.01.2017\u0026quot;, \u0026quot;04.01.2017\u0026quot;, \u0026quot;0‚Ä¶ ## $ place \u0026lt;fct\u0026gt; NS_Spens, NS_Spens, NS_Spens, NS_Spens, NS_Spens, NS_Spens‚Ä¶ ## $ no2 \u0026lt;dbl\u0026gt; 22.89, 32.94, 14.86, 22.73, 20.89, 10.47, 9.58, 15.99, 14.‚Ä¶  By now, you should have gained enough knowledge in using R that would give you that needed confidence to start exploring other functions of the tidyr package. You should not stop there, but go beyond and explore whole of the tidyverse opinionated collection of R packages for data science. üòáüé∂\n To learn more about tidy data in r chgeck Data Tidying section from the famous Data Science with R by Garrett Grolemund\nHave you tried learning data science by posting your questions and discussing it with other people within the R community? üë•üíªüìäüìàüó£ RStudio Community\n YOUR TURN üëá Practise by doing the following set of exercises:\n1) Install and upload the rattle package and see what it does.\n2) Create a new R script file to explore weatherAUS dataset.\ni) select() variable: MinTemp, MaxTemp, Rainfall and Sunshine by pipping the dataset into `dplyr::select() function.\nii) produce a summary using base::summary() function of these numeric values.\niii) within this selection filter only those observations where Rainfall \u0026gt;= 1 and save the results into the computer\u0026rsquo;s memory (ie. save the results as an object).\niv) Try to think of how else you can use other dplyr verbs on this weatherAUS dataset. Write your question first, before embarking on typing the code.\nv) Write a short report on what visualisation you think would be interesting to produce for this weatherAUS dataset and why?\nUseful Links Data Wrangling cheat sheet\nData Transformation with dplyr cheat sheet\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/general/whatlearn/",
	"title": "General Overview",
	"tags": [],
	"description": "",
	"content": " We live in a digital world where having access to the information at our fingertips doesn‚Äôt make it easier to communicate‚Ä¶ it makes it harder. In every data bundle there are many concealed narratives and conclusions, some stories will be unremarkable, others will be highly charged, but all remain out of sight without the skills required to view the hidden truths. Revealing unseen tales within data bundles and communicating that information requires curiosity, determination and skill. Applying the basic concepts, tools and related know how of data science procedures is key to deciphering and revealing the headlining facts. Naturally, in order for this to happen the basic concepts and practices of data analysis need to be applied and adopted by those wishing to uncover the stories. Telling stories with data is a skill that‚Äôs becoming ever more important in the digital world of increasing data and desire for informed data driven decision making.\nThe most viable software of choice for reporting modern data analysis is R, as data and code is reproducible and therefore useful to others, as all the information collated within the analysis is available. R is the lingua franca of quantitative research and as such is an influential and indispensable feature of any data scrutiny process. Through the use of grammar of graphics in R and Shiny, a web application framework for R, it is possible to develop interactive applications for graphical data visualisation. Presenting findings through interactive and accessible visualisation methods is an effective form of reaching audiences that might not otherwise realise the value of the topic or data at hand.\nThis course will provide an overview of key concepts for creating an effective data science project and will introduce tools and techniques for data wrangling, visualisation and dynamic reproducible reporting using R, a public domain language for data analysis. The R language provides a rich and flexible environment for working with data, especially data to be used for statistical modelling or graphics.\nThe R system has an extensive library of packages that offer state-of-the-art-abilities. Many of the analyses that they offer are not even available in any of the standard packages. R enables you to escape from the restrictive environments and sterile analyses offered by commonly used statistical software packages. It enables easy experimentation and exploration, which improves data analysis. Sharing your data analysis knowledge discovery is necessary in making it useful. R is a tool that enables reporting modern data analyses in a reproducible manner. It makes analysis more useful to others because the data and code that actually conducted the analysis can be made available and easily shared. As such R has become the lingua franca of quantitative research. Accordingly, this course will emphasize packages that will help you do data analysis, visualization and communication with the wider audience.\nThe course will start by introducing the fundamental concepts of R: basic use of R console through RStudio IDE, inputting and importing data, record keeping and general good practice of R project workflow. It will then progress to basic statistical concepts, which theoretically may be perceived as complex and thereby can be more effectively communicated by using visualisation. Hence, the formal abstract nature of Statistics can be demystified by visualising its application context, which is why the focus is directed on building appropriate visualisation of a given data analysis problem. At the end of the course, after students develop an understanding of their data they will use R‚Äôs reproducible and interactive approach to knit this into a tight and concise narrative and of course, present their story by creating reproducible RMarkdown documents and Shiny Web Apps.\nVersion control has become an essential tool for keeping track when working on DS projects, as well as collaborating. RStudio supports working with Git, an open source distributed version control system, which is easy to use when combined with GitHub, a web-based Git repository hosting service. We will introduce you to GitHub and you‚Äôll become acquainted with good practice when incorporating the use of Git into your R project workflow.\nThere is a demand for open and transparent data sources by governments and civic groups as a means to improve the lives of citizens. Together we will investigate the importance of open source data and we will identify where open source data can be readily found accross the Internet. You will work on case studies inspired by real problems and based on open data.\nObjectives:  To learn how to access and prepare data for the analysis\n To introduce the basic principles behind effective data visualization\n To learn essential explanatory techniques for summarising data\n To produce explanatory data visualizations providing insight into what could be found within the data\n To utilise R‚Äôs library of tools to visualise geospatial problems\n To design reproducible reports by automating the reporting process\n To share the results of analysis as interactive, eye-catching web apps that are friendly to non-programmers.\n To be familiar with R/RStudio‚Äôs data handling facilities that will expand the range of data analysis problems that can be effectively analysed.\n  How the course works The material is structured within 3 daily modules. Each module is a three and a halph hour long session split into 2¬Ω hours hands on interactive student/teacher workshops with the last hour reserved for questions and discussions.\nEach module will be taught by Dr Tatjana Kecojevic and will cover various related topics through appropriate case studies, presentations and readings. The conceptual models come to life when practice becomes reality during the hands on taught sessions, through the application of R. Students are then expected to use their own time to practice and hone acquired data handling expertise acquired during the taught sessions.\nStudents are expected to participate fully in all of these delivery modes, but in particular are expected to have attempted any pre-set work and come fully prepared to discuss any problems encountered and debate the ideas and any issues raised.\nWe recommend you complete each of the following before the end of each module:\n Readings and hand-outs/exercises Participation in the discussions Exercises covering concepts from tutorials and/or readings   Who can enrol This course is design for anyone who needs to communicate information to someone using data. It will benefit anyone who has the curiosity and desire to enter the realm of data exploration. We will seek to make sense of the world of data and learn effective and attractive ways to visually analyse and communicate related information. With the knowledge gained in this course, you will be ready to undertake your very first explanatory data analysis.\nData Science is not simply fashionable jargon, but rather a discipline with a set of tools that empower data enriched living, so whatever industry you‚Äôre in, this is relevant to you!\nPrior experience is not required.\nThe course will be delivered in English and Serbian!\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/whatisr/",
	"title": "What is R?",
	"tags": [],
	"description": "",
	"content": " Before there was R, there was S! ü§† R is a dialect of S language that was developed in 1976 by Rick Becker and John Chambers at the Bell Laboratories.\nRick Becker gave an excellent keynote talk \u0026ldquo;Forty Years of S\u0026rdquo; at UseR!2016 conference:\nRick Becker @ UseR!2016 where he talked about development of S language that gives explanations for many characteristics of R as we know it, including \u0026ldquo;\u0026lt;-\u0026rdquo; assignment operator.\n1993 Bell Labs gave StatSci (later Insightful Corp.) an exclusive license to develop and sell the S language. Insightful sold its implementation of the S language under the product name S-PLUS.\nYou can read more about the history of S, R, and S-PLUS\nThen, R was born üòáüé∂ In the early nineties at the University of Auckland in the Department of Statistics R was created by Ross Ihaka and Robert Gentleman.\nThey used GNU General Public License to make R open source free software.\nRoss Ihaka and Robert Gentleman. R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5(3):299‚Äì314, 1996\nCurrently R is developed by the R Development Core Team, of which John Chambers is a member.\nTo start using R you need to install it! üòÉ\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/general/",
	"title": "About the course",
	"tags": [],
	"description": "",
	"content": " About the Course\nThis course is designed to give you an appreciation of R programming as a tool for data exploration. It focuses on packages that will help you do exploratory data analysis and visualization and communication in a dynamic and reproducible manner. If you would like to:  Discover how to find and access data and prepare it for exploration and visualization Learn to explore, visualize, and analyse data in a dynamic and reproducible manner Gain experience in data wrangling, exploratory data analysis and data visualization, and effective communication of results; Work on the case studies inspired by real problems and based on open data  then this course is for you! üòÄ ¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day2/visualisation/",
	"title": "Data Visualisation",
	"tags": [],
	"description": "",
	"content": " Most of you, if not all, would be familiar with creating the graphs in Excel. Software such as Excel has a predefined set of menu options for plotting the data that is the focus of the end result: \u0026ldquo;pretty graph\u0026rdquo;. Those types of menus assume data to be in a format ready for plotting, which when you get raw data is hardly the case. You are probably going to have to organse and wrangle your data to make it ready for effective visualisation.\nGrammar of Graphics The grammer of graphics enables a structured way of creating a plot by adding the components as layers, making it look effective and attractive.\nIt enables you to specify building blocks of a plot and to combine them to create the graphical display that you want. There are 8 building blocks:\n data\n aesthetic mapping\n geometric object\n statistical transformations\n scales\n coordinate system\n position adjustments\n faceting\n  Imagine talking about baking a cake and adding a cherry on the top. üéÇüçí This philosophy has been built into the ggplot package by Hadle Wickham for creating elegant and complex plots in R.\nggplot2 Learning how to use the ggplot2 package can be challenging, but the results are highly rewarding and just like R itself, it becomes easier the more you use it.\nUnlike base graphics, ggplot works with dataframes and not individual vectors.\n The best way to master it is by practising. So let us create a first ggplot. üòÉ What we need to do is the following:\n i) Wrangle the data in the format suitable for visualisation.\n ii) \u0026ldquo;Initialise\u0026rdquo; a plot with ggplot():\n  ggplot(dataframe, aes(x = explanatory variable, y = resposne variable))\nThis will draw a blank ggplot, even though the x and y are specified. ggplot doesn‚Äôt assume the plot you meant to be drawn (a scatterplot). You only specify the data set and columns ie. variables to be used. Alos note that aes( ) function is used to specify the x and y axes.\n iii) Add layers with geom_ functions:  geom_point()\nWe will add points using a geom layer called geom_point.\n# load the packages suppressPackageStartupMessages(library(dplyr)) suppressPackageStartupMessages(library(gapminder)) suppressPackageStartupMessages(library(ggplot2)) # wrangle the data (Can you remember what this code do?) gapminder_pipe \u0026lt;- gapminder %\u0026gt;% filter(continent == \u0026quot;Europe\u0026quot; \u0026amp; year == 2007) %\u0026gt;% mutate(pop_e6 = pop / 1000000) # plot the data ggplot(gapminder_pipe, aes(x = pop_e6, y = lifeExp)) + geom_point(col =\u0026quot;red\u0026quot;)  ü§ìüí° Tip: You can use the following code template to make graphs with ggplot2:\n ggplot(data = \u0026lt;DATA\u0026gt;, (mapping = aes(\u0026lt;MAPPINGS\u0026gt;)) + \u0026lt;GEOM_FUNCTION\u0026gt;()  ggplot() gallery Run the following code to see what graphs it will produce.\nggplot(data = gapminder, mapping = aes(x = lifeExp), binwidth = 10) + geom_histogram() # ggplot(data = gapminder, mapping = aes(x = lifeExp)) + geom_density() # ggplot(data = gapminder, mapping = aes(x = continent, color = continent)) + geom_bar() # ggplot(data = gapminder, mapping = aes(x = continent, fill = continent)) + geom_bar()  üó£üë• Confer with your neighbours: Does the life expectancy depend upon the population size?\ny = b_0 + b_1 x + e\nRun this code in your console to fit the model pop vs lifeExp.\nPay attention to spelling, capitalization, and parentheses!\nm1 \u0026lt;- lm(gapminder_pipe$lifeExp ~ gapminder_pipe$pop_e6) summary(m1)  Can you answer the question using the output of the fitted model?\nm1 \u0026lt;- lm(gapminder_pipe$lifeExp ~ gapminder_pipe$pop_e6) summary(m1)  ## ## Call: ## lm(formula = gapminder_pipe$lifeExp ~ gapminder_pipe$pop_e6) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.324 -2.562 1.007 2.245 4.277 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 77.477421 0.721723 107.351 \u0026lt;2e-16 *** ## gapminder_pipe$pop_e6 0.008762 0.023779 0.368 0.715 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 3.025 on 28 degrees of freedom ## Multiple R-squared: 0.004826,\tAdjusted R-squared: -0.03072 ## F-statistic: 0.1358 on 1 and 28 DF, p-value: 0.7153  üëâ Practice ‚è∞üíª: Use gapminder data. Does the life expectancy depend upon the GDP per capita?\n1) Have a glance at the data. (tip: sample_n(df, n))\n2) Produce a scatterplot: what does it tell you?\n3) Fit a regression model: is there a relationship? How strong is it? Is the relationship linear? What conclusion(s) can you draw?\n4) What are the other questions you could ask; could you provide the answers to them?\nüòÉüôå Solution: code Q1; sample sample_n(gapminder, 30)  ## # A tibble: 30 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Burundi Africa 1987 48.2 5126023 622. ## 2 Uganda Africa 2007 51.5 29170398 1056. ## 3 Thailand Asia 1972 60.4 39276153 1524. ## 4 Mexico Americas 1982 67.4 71640904 9611. ## 5 Reunion Africa 1997 74.8 684810 6072. ## 6 Panama Americas 1967 64.1 1405486 4421. ## 7 Angola Africa 2002 41.0 10866106 2773. ## 8 Finland Europe 2002 78.4 5193039 28205. ## 9 Ecuador Americas 1982 64.3 8365850 7214. ## 10 Jamaica Americas 1992 71.8 2378618 7405. ## # ‚Ä¶ with 20 more rows  We will add layers onto this scatterplot: liveExp vs gdpPercap. We want to superimpose regression line of the best fit and non-parametric loess line that depict possible relationship between the two variables. That means we will have:\n 1st layer: scatterplot 2nd layer: line of the best fit 3rd layer: loess curve  üòÉüôå Solution: code Q2; Plot the data; ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.2, shape = 21, fill = \u0026quot;blue\u0026quot;, colour=\u0026quot;black\u0026quot;, size = 5) + # set transparency, shape, colour and size for points geom_smooth(method = \u0026quot;lm\u0026quot;, se = F, col = \u0026quot;maroon3\u0026quot;) + # change the colour of line geom_smooth(method = \u0026quot;loess\u0026quot;, se = F, col = \u0026quot;limegreen\u0026quot;) # change the colour of line  üòÉüôå Solution: code Q3; simple regression model my.model \u0026lt;- lm(gapminder_pipe$lifeExp ~ gapminder_pipe$gdpPercap) summary(my.model)  ## ## Call: ## lm(formula = gapminder_pipe$lifeExp ~ gapminder_pipe$gdpPercap) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.79839 -1.30472 0.00807 1.33443 2.87766 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 7.227e+01 6.942e-01 104.113 \u0026lt; 2e-16 *** ## gapminder_pipe$gdpPercap 2.146e-04 2.514e-05 8.537 2.8e-09 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1.598 on 28 degrees of freedom ## Multiple R-squared: 0.7225,\tAdjusted R-squared: 0.7125 ## F-statistic: 72.88 on 1 and 28 DF, p-value: 2.795e-09  Playing with the aesthetic: adding more layers to your ggplot() Whenever possible you should strive to make your graph visually appealing and informative as discussed in the previous section Principles of Visualisation.\nTo change the title and axis labels use layer labs labs(title = ‚Äúyour title‚Äù, subtitle = ‚Äúyour subtitle‚Äù, y = ‚Äúy label‚Äù, x = ‚Äúx label‚Äù, caption = ‚Äúgraph\u0026rsquo;s caption‚Äù)\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.2, shape = 20, col = \u0026quot;steelblue\u0026quot;, size = 5) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F, col = \u0026quot;maroon3\u0026quot;) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = F, col = \u0026quot;limegreen\u0026quot;) + # give a title an label axes labs(title = \u0026quot;Life Exp. vs. Population Size\u0026quot;, x = \u0026quot;population\u0026quot;, y = \u0026quot;Life Exp.\u0026quot;) + # modify the appearance theme(legend.position = \u0026quot;none\u0026quot;, panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;, size = .75), plot.title=element_text(hjust=0.5)) + # add the description geom_text(x = 80000, y = 125, label = \u0026quot;regression line\u0026quot;, col = \u0026quot;maroon3\u0026quot;) + geom_text(x = 90000, y = 75, label = \u0026quot;smooth line\u0026quot;, col = \u0026quot;limegreen\u0026quot;)  Note, that we have added text on the plot for the two lines and have edited the plot in terms of legend and its appearance.\nWe could also annotate the plot by using:\nannotate(\u0026quot;text\u0026quot;, x = 80000, y = 125 label = \u0026quot;regression line\u0026quot;, color = \u0026quot;maroon3\u0026quot;)  To learn more about how to modify the appearance of the theme go to ggplot‚Äôs theme page.\nChange the colour of the points to reflect categories of another, third variable. ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + # change the colour of the points to reflect continent it belongs to; set transparency, shape, and size for points geom_point(aes(col = continent), alpha = 0.5, shape = 20, size = 3) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F, col = \u0026quot;maroon3\u0026quot;) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = F, col = \u0026quot;dodgerblue3\u0026quot;) + labs (title= \u0026quot;Life Exp. vs. Population Size\u0026quot;, x = \u0026quot;population\u0026quot;, y = \u0026quot;Life Exp.\u0026quot;) + theme(legend.position = \u0026quot;right\u0026quot;, panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;, size = .75), plot.title=element_text(hjust=0.5)) + geom_text(x = 80000, y = 125, label = \u0026quot;regression line\u0026quot;, col = \u0026quot;maroon3\u0026quot;) + geom_text(x = 90000, y = 75, label = \u0026quot;smooth line\u0026quot;, col = \u0026quot;dodgerblue3\u0026quot;)  Note that the legend is added automatically. You can removed it by setting the legend.position to none from within a theme() function.\n Adjust the X and Y axis limits and chhange the X axis texts and its ticks\u0026rsquo; location ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(col = continent), alpha = 0.5, shape = 20, size = 3) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F, col = \u0026quot;maroon3\u0026quot;) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = F, col = \u0026quot;dodgerblue3\u0026quot;) + labs (title= \u0026quot;Life Exp. vs. Population Size\u0026quot;, x = \u0026quot;population\u0026quot;, y = \u0026quot;Life Exp.\u0026quot;) + theme(legend.position = \u0026quot;right\u0026quot;, panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;, size = .75), plot.title=element_text(hjust=0.5)) + geom_text(x = 48000, y = 90, label = \u0026quot;regression line\u0026quot;, col = \u0026quot;maroon3\u0026quot;) + geom_text(x = 70000, y = 75, label = \u0026quot;smooth line\u0026quot;, col = \u0026quot;dodgerblue3\u0026quot;) + # change the limits of the x \u0026amp; y axis xlim(c(0, 90000)) + ylim(c(25, 100))  ## Warning: Removed 5 rows containing non-finite values (stat_smooth). ## Warning: Removed 5 rows containing non-finite values (stat_smooth).  ## Warning: Removed 5 rows containing missing values (geom_point).  ## Warning: Removed 33 rows containing missing values (geom_smooth).  Note that the regression and smooth lines have changed their shapes üò≥‚Ä¶ all those warning üò¨ What‚Äôs going on?! üò≤\nWhen using xlim() and ylim(), the points outside the specified range are deleted and are not considered while drawing the line using geom_smooth(). This feature might come in handy when you wish to know how the line of best fit would change when some extreme values or outliers are removed.\n Thankfully, there is the other way to change the limits of the axis without deleting the points by simply zooming in to the region of interest. This is done using coord_cartesian(). You can try to replace xlim() and ylim() comands in the previous code chunk with the code below to see what would happen.\ncoord_cartesian(xlim = c(0, 90000), ylim = c(25, 100)) # zooming in specified limits of the x \u0026amp; y axis  You can set the breaks on the x axis and label them by using scale_x_continuous(). Similarly, you can do it for y axis too.\nTry to play with changing the colour palette. For more options check Sequential, diverging and qualitative colour scales from colorbrewer.org.\nThese are build-in themes which control all non-data display. You should use theme_bw() to have white background or theme_dark() for dark grey. For more build-in themes click here.\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(col = continent), alpha = 0.5, shape = 20, size = 3) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F, col = \u0026quot;maroon3\u0026quot;) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = F, col = \u0026quot;dodgerblue3\u0026quot;) + labs (title= \u0026quot;Life Exp. vs. Population Size\u0026quot;, x = \u0026quot;population\u0026quot;, y = \u0026quot;Life Exp.\u0026quot;) + theme(legend.position = \u0026quot;right\u0026quot;, panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;, size = .75), plot.title=element_text(hjust=0.5)) + geom_text(x = 80000, y = 125, label = \u0026quot;regression line\u0026quot;, col = \u0026quot;maroon3\u0026quot;) + geom_text(x = 90000, y = 75, label = \u0026quot;smooth line\u0026quot;, col = \u0026quot;dodgerblue3\u0026quot;) + # change breaks and label them scale_x_continuous(breaks = seq(0, 120000, 20000), labels = c(\u0026quot;0\u0026quot;, \u0026quot;20K\u0026quot;, \u0026quot;40K\u0026quot;, \u0026quot;60K\u0026quot;, \u0026quot;80K\u0026quot;, \u0026quot;10K\u0026quot;, \u0026quot;12K\u0026quot;)) + # change color palette scale_colour_brewer(palette = \u0026quot;Set1\u0026quot;) + # white background theme theme_bw()  There is a ggthemes library of themes that would help you create stylish ggplot charts used by different journals like Wall Street Journal or the Economist. See what are the other themes you can use by going to this website\n## If you don't have ggthemes installed yet, uncomment and run the line below #install.packeges(\u0026quot;ggthemes\u0026quot;) library(ggthemes) ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(col = continent), alpha = 0.5, shape = 20, size = 3) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F, col = \u0026quot;darkred\u0026quot;) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = F, col = \u0026quot;darkgreen\u0026quot;) + labs (title= \u0026quot;Life Exp. vs. Population Size\u0026quot;, x = \u0026quot;population\u0026quot;, y = \u0026quot;Life Exp.\u0026quot;) + theme(legend.position = \u0026quot;right\u0026quot;, panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;, size = .75), plot.title=element_text(hjust=0.5)) + geom_text(x = 80000, y = 125, label = \u0026quot;regression line\u0026quot;, col = \u0026quot;darkred\u0026quot;) + geom_text(x = 90000, y = 75, label = \u0026quot;smooth line\u0026quot;, col = \u0026quot;darkgreen\u0026quot;) + scale_x_continuous(breaks = seq(0, 120000, 20000), labels = c(\u0026quot;0\u0026quot;, \u0026quot;20K\u0026quot;, \u0026quot;40K\u0026quot;, \u0026quot;60K\u0026quot;, \u0026quot;80K\u0026quot;, \u0026quot;10K\u0026quot;, \u0026quot;12K\u0026quot;)) + # Wall Street Journal theme scale_colour_wsj() + theme_wsj()  You are ready to make a publication-ready data visualizations in R. üòé You can go further and explore for yourself to see if you could produce BBC style ggplot charts like those used in the BBC\u0026rsquo;s data journalism. Check out the BBC Visual and Data Journalism cookbook for R graphics.\nLay out panels in a grid Sometimes it might be hard to read one panel plot, like the one we have just created in which it is not very easy to see the points of each continent. To make it easier to follow and to understand the information you are trying to depict it would be more effective to present different categories of the same information in a clear set of multi-panel plots. This is easy to do by applying powerful faceting functions of the ggplot2: facet_wrap() and facet_grid().\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(col = continent), alpha = 0.5, shape = 20, size = 3) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F, col = \u0026quot;darkred\u0026quot;) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = F, col = \u0026quot;black\u0026quot;) + labs (title= \u0026quot;Life Exp. vs. Population Size\u0026quot;, x = \u0026quot;population\u0026quot;, y = \u0026quot;Life Exp.\u0026quot;) + theme(legend.position = \u0026quot;right\u0026quot;, panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;, size = .75), plot.title=element_text(hjust=0.5)) + scale_x_continuous(breaks = seq(0, 120000, 20000), labels = c(\u0026quot;0\u0026quot;, \u0026quot;20K\u0026quot;, \u0026quot;40K\u0026quot;, \u0026quot;60K\u0026quot;, \u0026quot;80K\u0026quot;, \u0026quot;10K\u0026quot;, \u0026quot;12K\u0026quot;)) + scale_colour_wsj() + theme_wsj() + # forms a matrix of scatterplots for each continet facet_grid(rows = vars(continent))  The main difference between facet_wrap() and facet_grid() is that the former can string together ggplots in different facets using a single variable, while the latter can do it for more than one.\nTry to explore the two functions for yourself and see where it would take you.\n üí™ There is a challenge:  dplyr\u0026rsquo;s group_by() function enables you to group your data. It allows you to create a separate df that splits the original df by a variable.\n boxplot() function produces boxplot(s) of the given (grouped) values.\n  Knowing about group_by() and boxplot() function, coud you compute the median life expectancy for year 2007 by continent and visualise your result?\nüòÉüôå Solution: code Let us look at the median life expectancy for each continent\ngapminder %\u0026gt;% group_by(continent) %\u0026gt;% summarise(lifeExp = median(lifeExp))  ## # A tibble: 5 x 2 ## continent lifeExp ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 47.8 ## 2 Americas 67.0 ## 3 Asia 61.8 ## 4 Europe 72.2 ## 5 Oceania 73.7  We are lucky that we live in Serbia, ie. Europe!!! üòÖ\nüòÉüôå Solution: graph Case study: NO2 2017 üòÅ Let\u0026rsquo;s try compbine everything we have learnt so far and practice using well nown to us 2017-NO2.csv data.\nRemember this?\nlibrary(tidyr) library(forcats) no2 \u0026lt;- read.csv(\u0026quot;http://data.sepa.gov.rs/dataset/ca463c44-fbfa-4de9-9a75-790995bf2830/resource/74516688-5fb5-47b2-becc-6b6e31a24d80/download/2017-no2.csv\u0026quot;, stringsAsFactors = FALSE, fileEncoding = \u0026quot;latin1\u0026quot;) new_no2 \u0026lt;- no2 %\u0026gt;% gather(\u0026quot;place\u0026quot;, \u0026quot;no2\u0026quot;, -Datum, factor_key = TRUE) %\u0026gt;% # stack all columns apart from `Datum` mutate(place = fct_recode(place, \u0026quot;NS_Spens\u0026quot; = \u0026quot;Novi.Sad.SPENS.NO2\u0026quot;, \u0026quot;BG_Most\u0026quot; = \u0026quot;Beograd.Mostar.NO2\u0026quot;, \u0026quot;BG_Vracar\u0026quot; = \u0026quot;Beograd.Vra√®ar.NO2\u0026quot;, \u0026quot;BG_ZelenoBrdo\u0026quot; = \u0026quot;Beograd.Zeleno.brdo.NO2\u0026quot;, \u0026quot;KG\u0026quot; = \u0026quot;Kragujevac..NO2\u0026quot;, \u0026quot;NI\u0026quot; = \u0026quot;Ni..IZJZ.Ni...NO2\u0026quot;, \u0026quot;UZ\u0026quot; = \u0026quot;U.ice..NO2\u0026quot;)) glimpse(new_no2)  ## Observations: 2,555 ## Variables: 3 ## $ Datum \u0026lt;chr\u0026gt; \u0026quot;01.01.2017\u0026quot;, \u0026quot;02.01.2017\u0026quot;, \u0026quot;03.01.2017\u0026quot;, \u0026quot;04.01.2017\u0026quot;, \u0026quot;0‚Ä¶ ## $ place \u0026lt;fct\u0026gt; NS_Spens, NS_Spens, NS_Spens, NS_Spens, NS_Spens, NS_Spens‚Ä¶ ## $ no2 \u0026lt;dbl\u0026gt; 22.89, 32.94, 14.86, 22.73, 20.89, 10.47, 9.58, 15.99, 14.‚Ä¶  new_no2 %\u0026gt;% group_by(place) %\u0026gt;% summarise(mean_no2 = mean(!is.na(no2))) %\u0026gt;% # !is.na(): is not NA; omits the missing values ggplot(aes(x = place, y = mean_no2, fill = place)) + # fill: colours each bar differently geom_bar(stat = \u0026quot;identity\u0026quot;) + xlab(\u0026quot;Place\u0026quot;) + scale_fill_brewer(palette = \u0026quot;Dark2\u0026quot;) + # colour scheme \u0026quot;Dark2\u0026quot; theme(legend.position=\u0026quot;bottom\u0026quot;, axis.text.x = element_blank(), axis.ticks.x = element_blank()) #  useful links: tidyverse, visualization, and manipulation basics\nIntroduction to R graphics with ggplot2\ngglopt cheat sheet\nfrom Data to Viz\nAn example from Financial Times\nBBC Visual and Data Journalism cookbook for R graphics\nhttp://johnburnmurdoch.github.io/slides/r-ggplot/#/\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/installr/",
	"title": "Install R/RStudio",
	"tags": [],
	"description": "",
	"content": " Install R To begin our journey to data science in R we need to install and make sure we can run it.\nWe will start by installing R first and then RStudio. The analogy we can use here is that R is an airplane and R Studio is its airport.\n R homepage - http://www.r-project.org CRAN Mirrors - http://cran.r-project.org  Run the installation\nYour R Counsole should look something like this:\nInstall RStudio RStudio is available in open source and commercial editions and runs on the desktop (Windows, Mac, and Linux).\nRStudio Desktop - http://www.rstudio.com\nRStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and work-space management. You can check its features by visiting RSTudio website.\nWe recomend you check regularly for R/Rstudio updates.\n ¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/general/instructor/",
	"title": "Meet the Instructor",
	"tags": [],
	"description": "",
	"content": "The instructor is Dr Tatjana Kecojeviƒá who is a longtime R user with a doctorate in Statistics from the University of Manchester. She spent many years working in the U.K. university sector as a senior lecturer and has published an extensive number of articles and papers in the field of quantile regression. Tatjana is the founder and co-organizer of R-Ladies Manchester, Belgrade and Novi Sad Chapters, leader of the R Forwards team and recently invited to become an ambassador for Women in DS (WiDS) ambassador. She is currently the founder and director of SisterAnalyst.org, an organisation aiming to empower women from a diverse range of backgrounds through data literacy. Unsurprisingly, Tatjana is a enthusiastic R user and in addition to her involvement supporting women in STEM related activities, she is dedicated to creating an inclusive culture by developing initiatives supporting all underrepresented groups within the DS community.\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/",
	"title": "Day 1",
	"tags": [],
	"description": "",
	"content": " Module 1 RStudio IDE; R language; Data classification and summary statistics; Introduction to Visualization Principles In this module you will set up the working environment and pass the first big hurdle of importing data and you will learn how to do it in the proper way with a command in R. You will learn how to use RStudio IDE for R from its installation to RStudio customisation and files navigation. You will learn good habits and practice of workflow in an R project. Once you get comfortable with the RStudio working environment you will move on to mastering the key features of R language and you will be introduced to fundamental principles behind effective data visualisation. What you will learn:  Basic use of R/RStudio console Good habits for workflow Inputting and importing different data types R environment: record keeping Data classification Descriptive summary statistics Basic principles of effective data visualisation  ¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/general/syllabus/",
	"title": "Indicative Syllabus",
	"tags": [],
	"description": "",
	"content": " Day 1 RStudio IDE; R language; Data classification and summary statistics; Introduction to Visualization Principles\nIn this module you will set up the working environment and pass the first big hurdle of importing data and you will learn how to do it in the proper way with a command in R. You will learn how to use RStudio IDE for R from its installation to RStudio customisation and files navigation. You will learn good habits and practice of workflow in an R project. Once you get comfortable with the RStudio working environment you will move on to mastering the key features of R language and you will be introduced to fundamental principles behind effective data visualisation.\nWhat you will learn:\n Basic use of R/RStudio console Good habits for workflow Inputting and importing different data types R environment: record keeping Data classification Descriptive summary statistics Basic principles of effective data visualisation  Day 2 Data Wrangling and Visualising Data\nIn this module you will learn some of the fundamental techniques for data exploration and transformation through the use of the dplyr package. This tidy verse package helps make your exploration intuitive to write and easy to read. You will learn dplyr‚Äôs key verbs for data manipulation that will help you uncover and shape the information within the data that is easy to turn into informative plots. Through the use of grammar of graphics plotting concepts implemented in the ggplot2 package, you will be able create meaningful exploratory plots. You will develop understanding about the way in which you should be able to think about necessary data transformations and summaries that can lead to an informative visualisation. You will learn how to create static maps and interactive maps with geolocated data by using the most popular packages in the R GIS community: simple features and leaflet.\nWhat you will learn:\n dplyr‚Äôs key data manipulation verbs: select, mutate, filter, arrange and summarise/summarize to aggregate data by groups to chain data manipulation operations using the pipe operator to specify ggplot2 building blocks and combine them to create graphical display about the philosophy that guides ggplot2: grammatical elements (layers) and aesthetic mappings. visualising data with maps  Day 3 Automated Reporting and Introduction to Shiny\nIn this module you will learn how to turn your analyses into high quality documents and presentations with R Markdown. You will be designing reproducible reports by automating the reporting process, learning how to take a modern approach to telling your data story. With the knowledge from this lesson you will be able to create reports straight from your R code allowing you to document your analysis and its results as an HTML, pdf, slideshow or Microsoft Word document. Afetr you gain fundamental knowledge of markdown and knitr, you will learn to create interactive web-graphics using Shiny R package.\nWhat you will learn:\n Authoring R Markdown Reports Embedding R Code knitr to compile dynamic R code LaTex to incorporate mathematical expressions create dynamic graphics using Shiny‚Äòs reactive features build and deploy Shiny app  ¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day2/",
	"title": "Day 2",
	"tags": [],
	"description": "",
	"content": " Module 2 Data Wrangling and Visualising Data In this module you will learn some of the fundamental techniques for data exploration and transformation through the use of the dplyr package. This tidy verse package helps make your exploration intuitive to write and easy to read. You will learn dplyr‚Äôs key verbs for data manipulation that will help you uncover and shape the information within the data that is easy to turn into informative plots. Through the use of grammar of graphics plotting concepts implemented in the ggplot2 package, you will be able create meaningful exploratory plots. You will develop understanding about the way in which you should be able to think about necessary data transformations and summaries that can lead to an informative visualisation. You will learn how to create static maps and interactive maps with geolocated data by using the most popular packages in the R GIS community: simple features and leaflet. What you will learn:  dplyr‚Äôs key data manipulation verbs: select, mutate, filter, arrange and summarise/summarize to aggregate data by groups to chain data manipulation operations using the pipe operator to specify ggplot2 building blocks and combine them to create graphical display about the philosophy that guides ggplot2: grammatical elements (layers) and aesthetic mappings. visualising data with maps  ¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/rstudioide/",
	"title": "RStudio IDE",
	"tags": [],
	"description": "",
	"content": " RStudio\u0026rsquo;s layout When you open RStudio for the first time it will be split into three sections. Each section has its own tab with shortcuts for the relevant options available from the main RStudio menu.\n The tall red section on the left is the Console and that‚Äôs where you can type in R code to execute. This code is also called commands or functions.\n In the top right section, there‚Äôs the Environment tab where you can see the data you are currently working on. At first this section is empty because you have not loaded any data yet.\n In the bottom right section there are tabs to flip through the Files and folder structure of your computer (like in Finder or Explorer), Help information etc.\n  ü§ìüí°: You might find it useful bookmarking the link for RStudio IDE Cheet Sheet!\n ¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/user/",
	"title": "How to use R?",
	"tags": [],
	"description": "",
	"content": " The RStudio window has mutiple panes. RStudio IDE Cheet Sheet:\nTop Left - Code Editor: for creating scripts to run in pieces or as a whole (like this document!);\nBottom Left - R Console: you can type R commands and see output;\nTop Right - Environment: lists the objects that you create, such as data sets;\nBottom Right - Help: find out information about functions and packages. This same panel will have tabs for showing plots that you make, view apps and documents, show files in the folder, and packages used.\nR is a scripting language, which means that it is just like writing an essay, or a math proof. We write a script to do specific tasks, that we can run again and again, or give to someone else to run.\nYou should take note of the following facts:\n R is case-sensitive language which executes instructions directly; Commands are entered at prompt \u0026gt; Commands are separate statements which could be put the in a same line if separated by a semi-colon ; Code statements can be commented by using a # tag. You can comment in continuation of the command line or in a separate line.  üí° The best way to learn is by doing. Therefore, I would like you not to copy paste the commands showing in the black boxes, but to type it in your R console. It would be even better to type it as an R script, so that you can keep a history of it and return to it if you wish to do so.\n Let\u0026rsquo;s start using R!  Turn on your computer (if you haven\u0026rsquo;t already) Open RStudio Create a project for your work. On the very right side of the window is a small blue R, and a drop-down menu. Select New project, then New directory, navigate to the desktop, and name the project My_First_R. This will create a folder with this name on the desktop. This will be your workspace for this project. üòá  Setting up your working directory If you want to read or write files on your computer from and to a specific location you will need to set a working directory in R. To set the working directory in R to a specific folder on your computer you will use the following:\n# On a pc, you would set working directory like this setwd(\u0026quot;C:/Documents/MyR_Project\u0026quot;) # On a mac, you would set working directory like this setwd(\u0026quot;~/documents/MyR_Project\u0026quot;)  ü§ìüí°: Make sure you fully adopt the correct syntax in terms of slashes and quotation marks.\n Note that the current working directory is displayed by RStudio within the title region of the Console. You can also setup your working directory by:\n selecting the options available from RStudio\u0026rsquo;s main menu  Use the Tools | Change Working Dir\u0026hellip; menu (Session | Set Working Directory on a mac).\n selecting the option from within the Files pane  Use the More | Set As Working Directory menu\nHowever, you should always start a fresh project (File | New Project\u0026hellip;) that would automatically set up your working directory without having to point it out to it in your script file. You should read Project-oriented workflow üíªüî• article by Jenny Bryan to convince yourself that this would be a good habit you should adopt.\nR Packages \u0026ldquo;In R, the fundamental unit of shareable code is the package.\u0026rdquo; Hadley Wickham, R packages\nR packages are collections of functions code, data sets, documentation and tests developed by the community, that are mostly made available on the Comprehensive R Archive Network, or CRAN, the public clearing house for R packages. Those pachages are developed by the experts in their fields and currently the CRAN package repository features over 14,000 of them. Many of the analyses that they offer are not even available in any of the standard data analysis software packages, which is one of the reasons that R is so successful.\nWhen you run R you will authomatically upload the package:base, which is the system library, i.e. the package where all standard functions are defined. The rest of the so called base packages contain the basic statistical routines. Assuming that you are connected to the internet, you can install a package using install.packages(). From RStudio menu you can do it by sellecting Tools | Install Packages\u0026hellip; and typing the name of the desired package in the dialogue window.\nOnce installed, the package will apear in the list of the available packages in your Packages panel. To use it you have to load it to the system‚Äôs search path by simply typing the name of the package as an argument of the library() function, or by checking the box next to its name from the Packages panel.\nüí°: Note that you can call the dialog window to install a package from Packages panel! Have you spotted the Install icon yet?\n Calculate in R To begin with, we can use R as a calculator. üòÅ\nIn your console type in 2 + 2. Note that you don‚Äôt have to type the equals sign and that answer has [1] in front. The [1] indicates that there is only one number in the answer. If the answer contains more than one number it uses numbering like this to indicate where in the ‚Äògroup‚Äô of numbers each one is.\nYou see?! R is like a big calculator! üò≤\nArithmetic and Logical Operators R‚Äôs binary and logical operators will look very familiar to those who have some experience with programming.\nArithmetic Operators Logical Operators Try doing other math operations, like subtraction, multiplication, division, or square root and power operations. For example, try the following:\n2 + 5 3 - 2 18 / 6 4 * 7 (5 - 3)^2 / 4 9^(1/2) * 4  Reproducibility: Save your scripts The code you type and want to be executed can be saved in scripts and R Markdown files. Scripts ending with .R file extension and R Markdown files, which mixes both R code and Markdown code, end with .Rmd.\nü§ìüí°: The code that you write just for quick exploration can be written in the console. Code we want to reuse and show off later should be saved as a script.\n To create a new script go through the menu File | New File | R Script or through the green plus button on the top left.\nAny code we type in here can be run and executed in the console. Hitting the Run button at the top of the script window will run the line of code on which the cursor is sitting.\nTo run multiple lines of code, highlight them and click Run.\nüí°: Get into a habit of saving your scripts after you create them. Try to save them before running your code in case you write code that makes R crash which sometimes happens. üò£ \u0026amp;@#$!?#%\n https://www.rstudio.com/products/rstudio/features/\nR Objects R provides a number of specialised data structures we will refer to as objects. To refer to an object we use a symbol. You can assign any object using the assignment operator \u0026lt;-, which is a composite made up from ‚Äòless than‚Äô and ‚Äòminus‚Äô, with no space between them! Thus, we can create scalar constants, which we refer to as variables, and perform mathematical operations over them.\nx \u0026lt;- 5 y \u0026lt;- 6  You can use objects in calculation in exactly the same way as as you have already seen numbers being used earlier:\nx + y  and you can store the results of the calculation done with the objects in another object:\nz \u0026lt;- x * y z  ü§ìüí° BUT, remember!!! Operator \u0026lt;- is a composite made up from ‚Äòless than‚Äô and ‚Äòminus‚Äô, with no space between them!!!!\n Try to type the following\nx\u0026lt; -5 y\u0026lt; -6  and see what happens.\nAfter you‚Äôve created some objects in R you can get a list of them using ls() function:\nls()  RStudio provides very comfortable working environment and enables you to monitor your list of objects in the Environment panel window in the top right corner.\nBuilt-In Functions R is not like other conventional statistical packages like SAS, Minitab, SPSS, to name a few. It is more of a programming language designed for conducting data analyses. It comes with a vast number of ready-made blocks of code that will enable you to manipulate data, perform intricate mathematical calculations with data, carry out an array of statistical analysis ranging from simple to complex to extremely complex and it will facilitate the creation of fantastic graphs. These pre-made blocks of code are known as functions.\nR has all the standard mathematical functions that you might ever need: sin, cos, tan, asin, atan, log, log10, exp, abs, sqrt, factorial‚Ä¶ To use them, all you need to do is to type the function and put the name of the object (argument) you would like to use the function for in brackets.\nYou should try a few üòÉ\nsqrt(144) log10(8) log10(100) log(100) exp(1) pi sin(pi/2) abs(-7) factorial(3) exp(x) log(y, 2)  You can use expression as argument of a function:\nz \u0026lt;- x * y trunc(x^2 + z / y) log((100 * x - y^2) / z)  You can have nested functions and you can use functions in creating new objects:\nround(exp(x), 2) p \u0026lt;- abs(floor(log((100*x - y^2) / exp(z)))) p  ü§ìüí° To obtain a description of a function you need to type a question mark, ?, in front of the name of the function. You might find this particularly useful when you start applying more complicated functions, as help will often provide you not only with the detailed description of the function‚Äôs input/output arguments, but practical illustrative examples on how the function can be used and applied. You should try to ask R for hlep for lm() function (can you tell what it is used for?)\n Vectors When analysing data you are more likely to be working with lots of numbers/variables. It would be much more convenient to keep all of those numbers/variables as an object. Variables can be of a different type: logical, integer, double, string are some examples. Variables with one or more values of the same type are vectors. Hence, a variable with a single value (known to us as a scalar) is a vector of length 1. We can assign to vectors in many different ways:\n generated by R using the colon symbol (:) as a sequence generated operator or by using built in function rep() for replicating the given number for a given number of times.\nx \u0026lt;- 1:10 x  ## [1] 1 2 3 4 5 6 7 8 9 10  x \u0026lt;- rep(1,10) x  ## [1] 1 1 1 1 1 1 1 1 1 1  generated by the user by using concatenation function c that allows you to enter one number at a the time\nx \u0026lt;- c(2, 6, 4, 2, 3, 7, 1, 5, 9, 8) x  ## [1] 2 6 4 2 3 7 1 5 9 8  created as a sequence of numbers. For example to generate a sequence of numbers from 1 to 10, with increments of 0.2 type\nseq(1,10,0.2)  ## [1] 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 ## [15] 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 ## [29] 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 9.2 ## [43] 9.4 9.6 9.8 10.0   üí°: R can easily perform arithmetic with vectors as it does with scalars. Thus, just as we can use those operators over the scalars we can use them when dealing with the vectors and/or a combination of both.\n x \u0026lt;- rep(1,10) y \u0026lt;- 1:10 x  ## [1] 1 1 1 1 1 1 1 1 1 1  y  ## [1] 1 2 3 4 5 6 7 8 9 10  c(x, y)  ## [1] 1 1 1 1 1 1 1 1 1 1 1 2 3 4 5 6 7 8 9 10  x + y  ## [1] 2 3 4 5 6 7 8 9 10 11  x + 2 * y  ## [1] 3 5 7 9 11 13 15 17 19 21  x^2 / y  ## [1] 1.0000000 0.5000000 0.3333333 0.2500000 0.2000000 0.1666667 0.1428571 ## [8] 0.1250000 0.1111111 0.1000000  z \u0026lt;- (x+y)/2 z  ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5  z \u0026lt;- c(z, rep(1, 3), c(100, 200, 300))+1 z  ## [1] 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 2.0 ## [12] 2.0 2.0 101.0 201.0 301.0  p \u0026lt;- 2.5 z*p  ## [1] 5.00 6.25 7.50 8.75 10.00 11.25 12.50 13.75 15.00 16.25 ## [11] 5.00 5.00 5.00 252.50 502.50 752.50  Accessing Vector‚Äôs Elements To access a specific element of a vector you would use index inside a single square bracket [] operator. The following shows how to obtain a vector member. The vector index is 1-based, thus use the index position 4 to access the fourth element.\nx \u0026lt;- c(9, 3, 7, 2, 9, 2, 1, 5, 4, 6) x  ## [1] 9 3 7 2 9 2 1 5 4 6  x[4]  ## [1] 2  üí°: In R you can evaluate functions over the entire vectors which helps to avoid looping.\n y \u0026lt;- c(4, 1, 0, 8, 1, x) max(y)  ## [1] 9  range(y)  ## [1] 0 9  mean(y)  ## [1] 4.133333  var(y)  ## [1] 9.409524  sort(y)  ## [1] 0 1 1 1 2 2 3 4 4 5 6 7 8 9 9  cumsum(y)  ## [1] 4 5 5 13 14 23 26 33 35 44 46 47 52 56 62  üí°: Note that missing values in R are represented by the symbol NA (not available) or NaN (not a number) for undefined mathematical operations.\n Here, NA would be shown if an index is out-of-range.\nz \u0026lt;- c(5, 8, 2) z[10]  ## [1] NA  You can also obtain a desirable selection of the elements of a vector by specifying a query within the index brackets []:\ny \u0026lt;- c(9, 3, 7, 2, 9, 2, 1, 5, 4, 6) y[y \u0026gt; 5]  ## [1] 9 7 9 6  y[y \u0026gt; 10]  ## numeric(0)  y[y != 2]  ## [1] 9 3 7 9 1 5 4 6  Matrices When data is arranged in two dimensions rather than one we have matrices. In R function matrix() creates matrices:\nma1 \u0026lt;- matrix(c(1, 0, -20, 0, 1, -15, 1, -1, 0), nrow = 3, ncol = 3, byrow = TRUE) ma1  ## [,1] [,2] [,3] ## [1,] 1 0 -20 ## [2,] 0 1 -15 ## [3,] 1 -1 0  dim(ma1)  ## [1] 3 3  The individual numbers in a matrix are called the elements of the matrix. Each element is uniquely defined by its particular row number and column number. To determine the dimensions of a matrix use function dim().\nAn element at the i-th row, j-th column of a matrix can be accessed by indexing inside square bracket operator [i, j]. The entire i-th row or entire j-th column of a matrix can be extracted as shown in the code below.\nma1 \u0026lt;- matrix(c(4, 10, 130, 0, 8, -7, 9, 11, -2, 7, -5, 4), nrow = 3, ncol = 4) ma1  ## [,1] [,2] [,3] [,4] ## [1,] 4 0 9 7 ## [2,] 10 8 11 -5 ## [3,] 130 -7 -2 4  ma1[2, 3]  ## [1] 11  ma1[1, ]  ## [1] 4 0 9 7  ma1[ , 4]  ## [1] 7 -5 4  Standard scalar algebra, which deals with operations on single numbers, has a set of well established rules for handling manipulations involving addition, subtraction, multiplication and division. In a broadly similar fashion a set of rules has been developed to enable us to manipulate matrices. However, introducing those rules is beyond the scope of this bootcamp, but they are nicely covered on the www.statisticshowto.com website.\nYOUR TURN üëá Practise by doing the following set of exercises:\n1) Create vectors called x1 and x2, where vector x1 consists of numbers: 1, 4, 7, 9, 11, 12, 13, 15 and 18 and vector x2 of numbers: 1, 1, 1, 2, 2, 2, 3, 3, 3.\n2) Subtract x2 from x1.\n3) Create a new vector called x3 by combining vectors x1 and x2.\n4) Calculate mean and variance of x3.\n5) Calculate medians for the three vectors.\n6) Create a matrix called m1 with the following elements: matrix(c(1, 5, 9, 2, 6, 10, 3, 7, 11, 4, 8, 12), nrow = 4, ncol = 3, byrow = TRUE)\n7) Use a subscript to find the 2-nd number in vector x1 and x2 and element in the 2-nd raw and 3-rd column in matrix m1.\n8) Add the 5-th number in vector x1 to the element in matrix m1 which is in 1-st raw and 1-st column.\n9) Calculate the mean of all numbers in x3 that are less than 13.\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/statsconcepts/",
	"title": "Basic Stats Concepts",
	"tags": [],
	"description": "",
	"content": " In this section you will be introduced to a set of concepts that enable data to be explored with the objective\n of summarising and understanding the main features of the variables contained within the data and to investigate the nature of any linkages between the variables that may exist.  The starting point is to understand what data is.\n What is the population? Why do we use samples?  Can you provide a formal definition about the population and the sample? üòÅ\nThe population is the set of all people/objects of interest in the study being undertaken.\nIn statistical terms the whole data set is called the population. This represents ‚ÄúPerfect Information‚Äù, however in practice it is often impossible to enumerate the whole population. The analyst therefore takes a sample drawn from the population and uses this information to make judgements (inferences) about the population.\nClearly if the results of any analysis are based on a sample drawn from the population, then if the sample is going to have any validity, then the sample should be chosen in a way that is fair and reflects the structure of the population.\nThe process of sampling to obtain a representative sample is a large area of statistical study. The simplest model of a representative sample is a \u0026ldquo;random sample\u0026rdquo;:\nA sample chosen in such a way that each item in the population has an equal chance of being included in the sample.\nAs soon as sample data is used, the information contained within the sample is ‚ÄúImperfect‚Äù and depends on the particular sample chosen. The key problem is to use this sample data to draw valid conclusions about the population with the knowledge of and taking into account the \u0026lsquo;error due to sampling\u0026rsquo;.\nThe importance of working with representative samples should be seriously considered; a good way to appreciate this importance is to see the consequences of using unrepresentative samples. A book by Darrell Huff called How to Lie with Statistics, published by Penguin contains several anecdotes of unrepresentative samples and the consequences of treating them as representative.\nData Analysis Using Sample Data Usually the data will have been collected in response to some perceived problem, in the hope of being able to glean some pointers from this data that will be helpful in the analysis of the problem. Data is commonly presented to the data analyst in this way with a request to analyse the data.\nBefore attempting to analyse any data, the analyst should:\ni) Make sure that the problem under investigation is clearly understood, and that the objectives of the investigation have been clearly specified.\nii) Before any analysis is considered the analyst should make sure that the individual variables making up the data set are clearly understood.\nThe analyst must understand the data before attempting any analysis.\nIn the summary you should ask yourself:\ni) Do I understand the problem under investigation and are the objectives of the investigation clear? The only way to obtain this information is to ask questions, and keep asking questions until satisfactory answers have been obtained.\nii) Do I understand exactly what each variable is measuring/recording?\nDescribing Variables A starting point is to examine the characteristics of each individual variable in the data set.\nThe way to proceed depends upon the type of variable being examined.\nClassification of variable types\nThe variables can be one of two broad types:\n Attribute variables: is a variable that has its outcomes described in terms of its characteristics or attributes.  gender days in a week    A common way of handling attribute data is to give it a numerical code. Hence, we often referred to them as coded variables.\n Measured variables: is a variable that has its outcomes taken from a numerical scale; the resulting outcome is expressed in numerical terms.\n weight age   There are two types of measured variables, a measured variable that is measured on some continuous scale of measurement, e.g. a person‚Äôs height. This type of measured variable is called a continuous variable. The other type of measured variable is a discrete variable. This results from counting; for example \u0026lsquo;the number of passengers on a given flight\u0026rsquo;.\nThe Concept of Statistical Distribution The concept of statistical distribution is central to statistical analysis.\nThis concept relates to the population and conceptually assumes that we have perfect information, the exact composition of the population is known.\nThe ideas and concepts for examining population data provide a framework for the way of examining data obtained from a sample. The Data Analyst classifies the variables as either attribute or measured and examines the statistical distribution of the particular sample variable from the sample data.\nFor an attribute variable the number of occurrences of each attribute is obtained, and for a measured variable the sample descriptive statistics describing the centre, width and symmetry of the distribution are calculated.\nattribute: measured: What does the distribution show? For an attribute variable it is very simple. We observe the frequency of occurance of each level of the attribute variable as shown in the barplot above.\nFor a measured variable the area under the curve from one value to another measures the relative proportion of the population having the oucome value in that range.\nA statistical distribution for a measured variable can be summarised using three key descriptions:\n the centre of the distribution; the width of the distribution; the symmetry of the distribution  The common measures of the centre of a distribution are the Mean (artithmetic average) and the Median. The median value of the variable is defined to be the particular value of the variable such that half the data values are less than the median value and half are greater\nThe common measures of the width of a distribution are the Standard Deviation and the Inter-Quartile Range. The Standard Deviation is the square root of the average squared deviation from the mean. Ultimately the standard deviation is a relative measure of spread (width), the larger the standard deviation the wider the distribution. The inter-quartile range is the range over which the middle 50% of the data values varies.\nBy analogy with the median it is possible to define the quartiles:\n Q1 is the value of the variable that divides the distribution 25% to the left and 75% to the right. Q2 is the value of the variable that divides the distribution 50% to the left and 50% to the right. This is the median by definition. Q3 is the value of the variable that divides the distribution 75% to the left and 25% to the right. The inter-quartile range is the value Q3 - Q1.  The diagram below shows this pictorially:\nü§ìüí° Conventionally the mean and standard deviation are given together as one pair of measures of location and spread, and the median and inter-quartile range as another pair of measures.\nThere are a number of measures of symmetry; the simplest way to measure symmetry is to compare the mean and the median. For a perfectly symmetric distribution the mean and the median will be exactly the same. This idea leads to the definition of Pearson\u0026rsquo;s coefficient of Skewness as:\nPearson's coefficient of Skewness = 3(mean - median) / standard deviation\nAn alternative measure of Skewness is the Quartile Measure of Skewness defined as:\nQuartile Measure of Skewness = [(Q1 - Q3) - (Q2 - Q1)]/(Q3 - Q1)\nImportant Key Points:  What is Data? Variables Two types of variable:\n an attribute variable a measured variable  The concept of a Statistical Distribution:\n As applied to an attribute variable As applied to a measured variable  Descriptive Statistics for a measured variable:\n Measures of Centre  Mean, Median  Measures of Width:  Standard Deviation Inter-Quartile Range    The descriptive statistics provide a numerical description of the key parameters of the distribution of a measured sample variable.\nInvestigating relationship between variables One of the key steps required of the Data Analyst is to investigate the relationship between variables. This requires a further classification of the variables contained within the data, as either a response variable or an explanatory variable.\nA response variable is a variable that measures either directly or indirectly the objectives of the analysis.\nAn explanatory variable is a variable that may influence the response variable.\nBivariate Relationships In general there are four different combinations of type of Response Variable and type of Explanatory Variable, these four combinations are shown below:\nThe starting point for any investigation of the connections between a response variable and an explanatory variable starts with examining the variables, and defining the response variable, or response variables, and the explanatory variables.\nü§ìüí°: In large empirical investigations there may be a number of objectives and a number of response variables.\nThe method for investigating the connection between a response variable and an explanatory variable depends on the type of variables. The methodology is different for combination as shown in the box above, and applying an inappropriate method causes problems. üí°‚ö°Ô∏èüò©\nDA Methodology The first step is to have a clear idea of what is meant by a connection between the response variable and the explanatory variable. This will provide a framework for defining a Data-Analysis process to explore the connection between the two variables, and will utilise the ideas previously developed.\nThe next step is to use some simple sample descriptive statistics to have a first look at the nature of the link between the variables. This simple approach may allow the analyst to conclude that on the basis of the sample information there is strong evidence to support a link, or there is no evidence of a link, or that the simple approach is inconclusive and further more sophisticated data analysis is required. This step is called the Initial Data Analysis and sometimes abbreviated to IDA.\nIf the Initial Data Analysis suggests that Further Data Analysis (FDA) is required, then this step seeks one of two conclusions:\ni) The sample evidence is consistent with there being no link between the response variable and the explanatory variable. or ii) The sample evidence is consistent with there being a link between the response variable and the explanatory variable.\nThe outcome of the analysis is one of the two alternatives given above. If the outcome is that there is no evidence of a connection, then no further action is required by the analyst since the analysis is now complete.\nIf however the outcome of the analysis is that there is evidence of a connection, then the nature of the connection between the two variables needs to be described.\nü§ìüí° The Data-Analysis Methodology described above seeks to find the answer to the following key question:\nOn the basis of the sample data is there evidence of a connection between the response variable and the explanatory variable?\nThe outcome is one of two conclusions\ni. No evidence of a relationship ii Yes there is evidence of a relationship, in which case the link needs to be described. This process can be represented diagrammatically as:\nFor each of the four data analysis situations given, the data analyst needs to know what constitutes the Initial Data Analysis (I.D.A.) and how to undertake and interpret the I.D.A. If Further Data Analysis is required the analyst needs to know how to undertake and interpret the Further Data Analysis.\nMeasured Vs Attribute(2-levels) There is a relationship between a measured response and an attribute explanatory variable if the average value of the response is dependent on the level of the attribute explanatory variable.\nGiven a measured response and an attribute explanatory variable with two levels, \u0026ldquo;red\u0026rdquo; \u0026amp; \u0026ldquo;blue\u0026rdquo;. If the statistical distribution of the response variable for attribute level \u0026ldquo;red\u0026rdquo; and attribute level \u0026ldquo;blue\u0026rdquo; are exactly the same then the level of the attribute variable has no influence on the value response, there is no relationship.\nThis can be illustrated as below:\nMeasured Vs Measured The first step is to have a clear idea of what is meant by a connection between a measured response variable and a measured explanatory variable. Imagine a population under study consisting of a very large number of population members, and on each population member two measurements are made, the value of Y the response variable and the value of X the explanatory variable. For the whole population a graph of Y against X could be plotted conceptually.\nIf the graph shows a perfect line, then there is quite clearly a link between Y and X. If the value of X is known, the exact value of Y can be read off the graph. This is an unlikely scenario in the data-analysis context, because this kind of relationship is a deterministic relationship. Deterministic means that if the value of X is known then the value of Y can be precisely determined from the relationship between Y and X. What is more likely to happen is that other variables may also have an influence on Y.\nIf the nature of the link between Y and X is under investigation then this could be represented as:\nY = f(X) + effect of all other variables [effect of all other variables is commonly abbreviated to e]\nConsidered the model:\nY = f(X) + e [e is the effect of all other variables]\nThe influence on the response variable Y can be thought of as being made up of two components:\ni) the component of Y that is explained by changes in the value of X, [the part due to changes in X through f(X)]\nii) the component of Y that is explained by changes in the other factors. [the part not explained by changes in X]\nOr in more abbreviated forms the \u0026lsquo;Variation in Y Explained by changes X\u0026rsquo; or \u0026lsquo;Explained Variation\u0026rsquo; and the \u0026lsquo;Variation in Y not explained by changes in X\u0026rsquo; or the \u0026lsquo;Unexplained Variation\u0026rsquo;.\nIn conclusion, the Total Variation in Y is made up of the two components:\n the \u0026lsquo;Changes in Y Explained by changes in X\u0026rsquo; and\n the \u0026lsquo;Changes in Y not explained by changes in X\u0026rsquo;  Which may be written as:\n`'The Total Variation in Y' = 'Explained Variation' + 'Unexplained Variation'`  ü§ìüí° The discussion started with the following idea:\nY = f(X) + e\nAnd to quantify the strength of the link the influence on Y was broken down into two components:\n'The Total Variation in Y' = 'Explained Variation' + 'Unexplained Variation'  This presents two issues: A: Can a model of the link be made? B. Can \u0026lsquo;The Total Variation in Y\u0026rsquo;, \u0026lsquo;Explained Variation\u0026rsquo; and the \u0026lsquo;Unexplained Variation\u0026rsquo; be measured?\nWhat do these quantities tell us?\nMaybe we can observe a propotion of the \u0026lsquo;Explained Variation in Y\u0026rsquo; over the \u0026lsquo;Total Variation in Y\u0026rsquo;. This ratio is always on the scale 0 to 1, but by convention is usually expressed as a percentage so is regarded as on the scale 0 to 100%. It is called \u0026lsquo;R Squared\u0026rsquo; and interpretation of this ratio is as follows:\nR_sq: 0% (no link) \u0026lt;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; 50% (Statistical Link) \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026gt; 100% (Perfect Link)\nThe definition and interpretation of R_sq is a very important tool in the data analyst tool kit for tracking connections between a measures response variable and a measured explanatory variable.\nWe can put those ideas into our DA Methodology frameworks as shown below.\nü§ìüí° Note that you will hardly ever be in the situation in which R_sq would be so close to zero that would make you conclude that on the base of the sample evidence used in IDA it is possible to conclude that there is no relationship between the two variables. If R_sq value is very small (for example around 2%) this would need to be further tested to conclude if it is statistically significant based on the sample evidence by applying FDA.\nFurther Data Analysis If the \u0026lsquo;Initial Data Analysis\u0026lsquo; is inconclusive then \u0026lsquo;Further Data Analysis\u0026lsquo; is required.\nThe \u0026lsquo;Further Data Analysis\u0026rsquo; is procedure that enables a decision to be made, based on the sample evidence, as to one of two outcomes:\n- There is no relationship - There is a relationship\nThese statistical procedures are called hypothesis tests, which essentially provide a decision rule for choosing between one of the two outcomes: \u0026ldquo;There is no relationship\u0026rdquo; or \u0026ldquo;There is a relationship\u0026rdquo; based on the sample evidence.\nAll hypothesis tests are carried out in four stages: - Stage 1: S pecifying the hypotheses.\n Stage 2: Defining the test parameters and the decision rule.\n Stage 3: Examining the sample evidence.\n Stage 4: The conclusions.\n  Statistical Models used in FDA  Measured Response v Attribute Explanatory Variable with exactly two levels:\n t-test  Measured Response v Attribute Explanatory Variable with more than two levels:\n One-Way ANOVA   Measures Response v Measured Explanatory Variable\n Simple Regression Model   Measures Response v Measured Explanatory Variables\n Multifactor Regression Model  Attribute Response v Attribute Explanatory Variable\n Chi-Square Test of Independence   YOUR TURN üëá Make sure you can answer the following questions:\n1) What are the underlying ideas that enable a relationship between two variables to be investigated?\n2) What is the purpose of summary statistics?\n3) What is the data analysis methodology for exploring the relationship between:\ni) a measured response variable and an attribute explanatory variable?\ni) a measured response variable and a measured explanatory variable?\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/datatypes/",
	"title": "Data Types",
	"tags": [],
	"description": "",
	"content": " The examples we have used in \u0026lsquo;How to Use R\u0026rsquo; section are all dealing with numbers (quantitative numerical data). Those of you familiar with programming will know that numerical objects can be classified as real, integer, double or complex. To check if an object is numeric and what type it is, you can use mode() and class() functions respectively.\nLet us go back into our R project and type the following into the open script file and run the code.üòÉ\nx \u0026lt;- 1:10 mode(x)  ## [1] \u0026quot;numeric\u0026quot;  class(x)  ## [1] \u0026quot;integer\u0026quot;  In R to enter strings of characters as objects you need to enter them using quote marks around them. By default it expects all inputs to be numeric and unless you use quote marks around the strings you wish to enter, it will consider them as numbers and subsequently will return an error message.\nx \u0026lt;- c(\u0026quot;UK\u0026quot;, \u0026quot;Spain\u0026quot;, \u0026quot;Serbia\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;Italy\u0026quot;) mode(x)  ## [1] \u0026quot;character\u0026quot;  It is common in statistical data to have attributes also known as categorical variables. In R such variables should be specified as factors. Attribute variable has a set of levels indicating possible outcomes. Hence, to deal with x as an attribute variable with five levels we need to make it a factor in R.\nx \u0026lt;- c(\u0026quot;UK\u0026quot;, \u0026quot;Spain\u0026quot;, \u0026quot;Serbia\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;Italy\u0026quot;) x \u0026lt;- factor(x) x  ## [1] UK Spain Serbia France Germany Italy ## Levels: France Germany Italy Serbia Spain UK  üí°: Note that R codes the factor levels in their alphabetical order. However, attribute variables are usual coded and you would usually enter them as such.\n quality \u0026lt;- factor(c(3, 3, 4, 2, 2, 4, 0, 1)) levels(quality)  ## [1] \u0026quot;0\u0026quot; \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot;  quality  ## [1] 3 3 4 2 2 4 0 1 ## Levels: 0 1 2 3 4  You might need to deal from time to time with logical data type. This is when something is recorded as TRUE or FALSE. It is most likely that you would use this data type when checking what type of data the variable is that you are dealing with. For example\nx \u0026lt;- c(\u0026quot;UK\u0026quot;, \u0026quot;Spain\u0026quot;, \u0026quot;Serbia\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;Italy\u0026quot;) is.numeric(x)  ## [1] FALSE  is.factor(x)  ## [1] FALSE  Data Frames Statistical data usually consists of several vectors of equal length and of various types that resemble a table. Those vectors are interconnected across so that data in the same position comes from the same experimental unit, ie. observation. R uses data frame for storing this kind of data table and it is regarded as primary data structure.\nLet us consider a study of share prices of companies from three different business sectors. As part of the study a random sample (n=15) of companies was selected and the following data was collected:\nshare_price \u0026lt;- c(880, 862, 850, 840, 838, 799, 783, 777, 767, 746, 692, 689, 683, 661, 658) profit \u0026lt;- c(161.3, 170.5, 140.7, 115.7, 107.9, 135.2, 142.7, 114.9, 110.4, 98.9, 90.2, 80.6, 85.4, 91.7, 137.8) sector \u0026lt;- factor(c(3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, + 1, 1)) # 1:IT, 2:Finance, 3:Pharmaceutical # share_price  ## [1] 880 862 850 840 838 799 783 777 767 746 692 689 683 661 658  profit  ## [1] 161.3 170.5 140.7 115.7 107.9 135.2 142.7 114.9 110.4 98.9 90.2 ## [12] 80.6 85.4 91.7 137.8  sector  ## [1] 3 3 3 3 3 2 2 2 2 2 1 1 1 1 1 ## Levels: 1 2 3  Rather than keeping this data as a set of individual vectors in R, it would be better to keep whole data as a single object, i.e. data frame.\nshare.data \u0026lt;- data.frame(share_price, profit, sector) share.data  Individual vectors from the data frame can be accessed using $ symbol:\nshare.data$sector  Now, as we have mastered the basics let us learn how to access existing data from R.\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/importexport/",
	"title": "Import Data inot R",
	"tags": [],
	"description": "",
	"content": " It‚Äôs the first big hurdle to dealing with data in R.\nYou are most likelly to have looked at, organised and examined your data files in Excel.\nOpening data in R is fairly simply, but organising it for analysis might take some thought and consideration. You\u0026rsquo;ll guess that it is possible to use File | Import Dataset menu option in RStudio to import your data (to learn more see Importing Data with RStudio), however we‚Äôre going to do it the proper way with a command.\nR can import all types of data:\n (Tab, Blank space) Delimited Text CSV files Excel files JSON SAS STATA MiniTab SPSS\u0026hellip;  In this section we will show you how to access most commonly used data types. Once you start playing with the data and wrangling it in the way it suits you for your analytical manipulations you might find interesting exploring the options for writing data. Here, we are not going to show you how to do it, but rather focus on basic read data functionality in R.\nthe R base The simplest data form is a text file. Text files can be read by any computer operating system and by almost all different kind of statistical programs. Saving data as a text file makes your data highly accessible. If you have a text data file you wish to use, you can easily import it with the R base functions read.table()\nReads a file in table format and creates a data frame from it\n# Read tabular data into R df_txt \u0026lt;- read.table(file_name.txt, header = FALSE)  There are many packages that let R import all types of data, and we will start by focus on CSV files as they are the most frequent tipe.\nComma separated files are the most common way to save spreadsheets that don‚Äôt require a licenced, usually not free program to open. Importing CSV is part of base R and you can use read.csv() function to do so.\nread.csv() Let us go to https://data.gov.rs/ Serbian open data portal. In particular, let us try to access Kvalitet vazduha 2017: pm2.5_2017.csv and import this data to R.\nWe are not going to download it onto our local computers, but rather we will import it to R directly from the web using the provided link.\ndf_csv \u0026lt;- read.csv(\u0026quot;http://data.sepa.gov.rs/dataset/ca463c44-fbfa-4de9-9a75-790995bf2830/resource/62983880-6fcd-4c65-b57c-fd4de5c367c8/download/pm2.5_2017.csv\u0026quot;, stringsAsFactors = FALSE) head(df_csv, 10)  ## Datum Nis_IZJZ.Nis ## 1 1.1.2017 132.04583 ## 2 2.1.2017 124.95417 ## 3 3.1.2017 79.92500 ## 4 4.1.2017 107.22500 ## 5 5.1.2017 42.14583 ## 6 6.1.2017 18.12500 ## 7 7.1.2017 25.14167 ## 8 8.1.2017 67.86667 ## 9 9.1.2017 41.41667 ## 10 10.1.2017 55.75417  The other way is to download it and to save it to your working directory.\nLet us download \u0026ldquo;Kvalitet vazduha 2017-SO2: 2017-SO2.csv\u0026rdquo; Make sure you save the file into your R-Project working directory before you ask R to execute the following\ndf_csv \u0026lt;- read.csv(\u0026quot;2017-so2.csv\u0026quot;, stringsAsFactors = FALSE, fileEncoding = \u0026quot;latin1\u0026quot;)  Run the above code again, but remove stringsAsFactors argument from the read.csv function or set it to TRUE. Can you tell the difference between now and before? Why do you think we had to use fileEncoding arguent?\n What do you think about this dataset? ü§î\nUsing readr::read_csv() readr is a package that read rectangular data quicker then when using base R read.cdv(). Another difference from the read.csv() function is that it assumes characters are strings and not factors by default.\nJust so you can see how easy it is to use other packages for importing data into R the code below illustrates the use of read_csv().\n## If you don't have readr installed yet, uncomment and run the line below # install.packages(\u0026quot;readr\u0026quot;) library(readr) df_csv \u0026lt;- read_csv(\u0026quot;air_quality_by_station.csv\u0026quot;) df_csv  Check the readr::read_delim() function for more efficient delimited data file reading.\n Importing Excel files with readxl Importing Excel data file is not straight forward as it might contain multiple sheets. We will focus on using the readxl package as it is the most efficent so far.\nTo access the data from an Excel sheet you can‚Äôt just copy and paste the URL for the file. You have to download the file first.\n Let us download an Excel data file from data.gov.rs about the traffic accidents –ü–æ–¥–∞—Ü–∏ –æ —Å–∞–æ–±—Ä–∞—õ–∞—ò–Ω–∏–º –Ω–µ–∑–≥–æ–¥–∞–º–∞ –¥–æ 31.08.2019. –≥–æ–¥–∏–Ω–µ –∑–∞ —Ç–µ—Ä–∏—Ç–æ—Ä–∏—ò—É —Å–≤–∏—Ö –ü–û–õ–ò–¶–ò–à–°–ö–ò–• –£–ü–†–ê–í–ê –∏ –û–ü–®–¢–ò–ù–ê.\nIf you are unable to open the file in Excel to examin how many sheets the file has, try to read the file in R accessing the first one by specifying it in read_excel() function as required. As previous, make sure you save the file into your R-Project working directory before you ask R to execute the following:\n## If you don't have readxl installed yet, uncomment the line below and run it #install.packages(\u0026quot;readxl\u0026quot;) library(readxl) df_xl \u0026lt;- read_excel(\u0026quot;nez-opendata-20199-20190925.xlsx\u0026quot;, sheet = 1)  What do you think?\nPeople like to make their Excel spreadsheet looks \u0026lsquo;pretty\u0026rsquo; with \u0026lsquo;fancy\u0026rsquo; formatting, which could create difficulty when reading them in R.\n Explore the arguments of read_excel() function, such as the skip argument through which you can specify a minimum number of rows to skip before reading anything.\nImporting data using jsonlite When accessing JSON file in R using the jsonlite package you need to point to the file by providing the local path if the file is downloaded or by the URL from where it could be accessed.\n## If you don't have readxl installed yet, uncomment the line below and run it #install.packages(\u0026quot;jsonlite\u0026quot;) library(jsonlite) my_url \u0026lt;-\u0026quot;https://data.gov.rs/sr/datasets/r/41c2fe91-4ea1-4a64-b33c-183665ea6ab3\u0026quot; polen \u0026lt;- fromJSON(my_url)  Check the structure of the polen! üò∞\nstr(polen)  ## List of 4 ## $ count : int 25888 ## $ next : chr \u0026quot;http://polen.sepa.gov.rs/api/opendata/pollens/?page=2\u0026quot; ## $ previous: NULL ## $ results :'data.frame':\t500 obs. of 4 variables: ## ..$ id : int [1:500] 539 805 1078 1351 1624 1897 2170 2686 2965 3238 ... ## ..$ location : int [1:500] 12 3 4 20 5 10 14 13 17 9 ... ## ..$ date : chr [1:500] \u0026quot;2016-02-01\u0026quot; \u0026quot;2016-02-01\u0026quot; \u0026quot;2016-02-01\u0026quot; \u0026quot;2016-02-01\u0026quot; ... ## ..$ concentrations:List of 500 ## .. ..$ : int [1:4] 3002 3003 3004 3005 ## .. ..$ : int [1:3] 4649 4650 4651 ## .. ..$ : int [1:3] 6126 6127 6128 ## .. ..$ : int(0) ## .. ..$ : int(0) ## .. ..$ : int [1:2] 10767 10768 ## .. ..$ : int [1:3] 12370 12371 12372 ## .. ..$ : int [1:2] 15518 15519 ## .. ..$ : int [1:4] 17422 17423 17424 17425 ## .. ..$ : int [1:2] 19339 19340 ## .. ..$ : int [1:4] 21104 21105 21106 21107 ## .. ..$ : int [1:3] 24123 24124 24125 ## .. ..$ : int [1:2] 26010 26011 ## .. ..$ : int 29460 ## .. ..$ : int [1:2] 1480 1481 ## .. ..$ : int [1:4] 3006 3007 3008 3009 ## .. ..$ : int 4652 ## .. ..$ : int [1:2] 6129 6130 ## .. ..$ : int(0) ## .. ..$ : int(0) ## .. ..$ : int [1:4] 10769 10770 10771 10772 ## .. ..$ : int [1:3] 12373 12374 12375 ## .. ..$ : int [1:5] 15520 15521 15522 15523 15524 ## .. ..$ : int [1:5] 17426 17427 17428 17429 17430 ## .. ..$ : int [1:3] 19341 19342 19343 ## .. ..$ : int [1:3] 21108 21109 21110 ## .. ..$ : int [1:3] 24126 24127 24128 ## .. ..$ : int [1:3] 26012 26013 26014 ## .. ..$ : int [1:3] 29461 29462 29463 ## .. ..$ : int [1:5] 1482 1483 1484 1485 1486 ## .. ..$ : int [1:3] 3010 3011 3012 ## .. ..$ : int [1:3] 4653 4654 4655 ## .. ..$ : int [1:2] 6131 6132 ## .. ..$ : int(0) ## .. ..$ : int(0) ## .. ..$ : int [1:3] 10773 10774 10775 ## .. ..$ : int [1:3] 12376 12377 12378 ## .. ..$ : int [1:3] 15525 15526 15527 ## .. ..$ : int [1:4] 17431 17432 17433 17434 ## .. ..$ : int [1:3] 19344 19345 19346 ## .. ..$ : int [1:5] 21111 21112 21113 21114 21115 ## .. ..$ : int [1:6] 24129 24130 24131 24132 24133 24134 ## .. ..$ : int [1:4] 26015 26016 26017 26018 ## .. ..$ : int [1:3] 29464 29465 29466 ## .. ..$ : int [1:3] 1487 1488 1489 ## .. ..$ : int 3013 ## .. ..$ : int [1:3] 4656 4657 4658 ## .. ..$ : int [1:3] 6133 6134 6135 ## .. ..$ : int(0) ## .. ..$ : int 9357 ## .. ..$ : int [1:5] 10776 10777 10778 10779 10780 ## .. ..$ : int [1:3] 12379 12380 12381 ## .. ..$ : int [1:3] 15528 15529 15530 ## .. ..$ : int [1:2] 17435 17436 ## .. ..$ : int [1:3] 19347 19348 19349 ## .. ..$ : int [1:2] 21116 21117 ## .. ..$ : int [1:3] 24135 24136 24137 ## .. ..$ : int [1:3] 26019 26020 26021 ## .. ..$ : int [1:3] 29467 29468 29469 ## .. ..$ : int [1:3] 1490 1491 1492 ## .. ..$ : int 3014 ## .. ..$ : int [1:2] 4659 4660 ## .. ..$ : int [1:3] 6136 6137 6138 ## .. ..$ : int(0) ## .. ..$ : int 9358 ## .. ..$ : int [1:3] 10781 10782 10783 ## .. ..$ : int [1:3] 12382 12383 12384 ## .. ..$ : int 15531 ## .. ..$ : int [1:3] 17437 17438 17439 ## .. ..$ : int [1:2] 19350 19351 ## .. ..$ : int [1:5] 21118 21119 21120 21121 21122 ## .. ..$ : int [1:3] 24138 24139 24140 ## .. ..$ : int [1:3] 26022 26023 26024 ## .. ..$ : int [1:4] 29470 29471 29472 29473 ## .. ..$ : int [1:3] 1493 1494 1495 ## .. ..$ : int [1:2] 3015 3016 ## .. ..$ : int 4661 ## .. ..$ : int [1:3] 6139 6140 6141 ## .. ..$ : int 7747 ## .. ..$ : int 9359 ## .. ..$ : int [1:3] 10784 10785 10786 ## .. ..$ : int [1:3] 12385 12386 12387 ## .. ..$ : int [1:3] 15532 15533 15534 ## .. ..$ : int [1:3] 17440 17441 17442 ## .. ..$ : int [1:3] 19352 19353 19354 ## .. ..$ : int [1:3] 21123 21124 21125 ## .. ..$ : int [1:4] 24141 24142 24143 24144 ## .. ..$ : int [1:3] 26025 26026 26027 ## .. ..$ : int [1:3] 29474 29475 29476 ## .. ..$ : int [1:5] 1496 1497 1498 1499 1500 ## .. ..$ : int [1:2] 3017 3018 ## .. ..$ : int 4662 ## .. ..$ : int [1:3] 6142 6143 6144 ## .. ..$ : int(0) ## .. ..$ : int [1:2] 9360 9361 ## .. ..$ : int [1:3] 10787 10788 10789 ## .. ..$ : int 12388 ## .. ..$ : int [1:5] 15535 15536 15537 15538 15539 ## .. ..$ : int [1:3] 17443 17444 17445 ## .. .. [list output truncated]  names(polen)  ## [1] \u0026quot;count\u0026quot; \u0026quot;next\u0026quot; \u0026quot;previous\u0026quot; \u0026quot;results\u0026quot;  Note that the polen$results is a data frame with a list concentrations inside as its elemnt.\nOuch! üò≥ JSON files are not very neat üò± They are more than often nested, chained -\u0026gt; you‚Äôve got it: Very Messy! üò´ So, we will leave it there. üò¨ If you do need to learn more about reading JSON files in R you can explore the functionality of the jsonlite package further by reading Getting started with JSON and jsonlite. Blog post Working with JSON data in very simple way by Kan Nishida provides a great example of how this data format can be used in R.\nOther data formats To speed up the reading process od txt, csv data files you can use the data.table::fread() function. You should only pass to the funtion the name of the data file you want to import, and fread() will try to work out the appropriate data structure. Check out this blog post Getting Data From An Online Source for some more ideas.\nYou can use R with appropriate packages to access other data formats. The haven package provides functions for importing SAS, SPSS and Stata file formats or you can use foreign package for MiniTab portable worksheet data files. Try to look through the help of the packages you\u0026rsquo;ve been introduced to and discover other options.\nYOUR TURN üëá 1) See if you can find data from https://data.gov.rs/ about a topic you‚Äôre interested in.\n2) Have a look at this data set: saobracaj. Think about the questions you can answer based on this data.\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/day1/principlesvisualisation/",
	"title": "Introduction to Visualization Principles",
	"tags": [],
	"description": "",
	"content": " Designed to enable: exploration, discovery, communication Graphical methods are commonly used for exploratory data analysis. Boxplots, scatterplot matrices, nonparametric smoothers and tree diagrams are just some of the mostly used graphical tools for data exploration. This part of the course will provide practical recommendation for choosing the best chart or graph type for creating good and clear graphics that would serve the purpose of communicating the key information\n to intended audience and the publication to help readers answer the questions your graphic is showing to understand it easy without getting bewildered\n  Graphical Forms::Encoding Good and clear graphics rely most of all on a reliable data. Thus, the first principle of an effective visualisation is that it represents reliable information. The type of the information that needs to be communicated and displayed will direct the choice of the most appropriate type of data encoding that would make relevant patterns become noticeable. It is therefore important to understand the problem you wish to communicate and the type of the data you need for its communication from the statistical perspective, ie. is it measured, categorical (ordinal or nominal), time (temporal dimension) and geographical location (spatial dimensions) in case of spatiotemporal data.\n\nVisual encoding of a data set depends on the number and characteristics of the available attributes ie. variables and on the analytical problem in question. Alberto Cairo on his blog The Functional Art provides an effective list of the graphic forms used to encode data depending on the function of the display. The figure shows ranking of the elementary perception task according on how well they can be perceived based on the ground breaking work of Cleveland and McGill published in the paper of JASA while working in the famous AT\u0026amp;T Bell Labs. The above figure illustrates the order in which graphical forms could be placed based on the accuracy of the conclusions which readers can draw about the given data from them. If, for example, the goal of the graphic is to facilitate precise comparisons, Alberto in his book The Functional Art provides an effective illustration of superiorities between possible choices of the graphical forms that could be used.\nThere is not a specific methodology that is develop for choosing the most appropriate ways of encoding data. You never know if a visual form will work until you give it a try. It mostly depends on what your attributes are that you using in order to revel that special something from the data. However, there are some useful guidelines made by a few authors of which I would recommend to check:\n The Data Visualisation Catalogue by Severino Ribecca Chart Chooser by Depict Data Studio and Visual Vocabulary by by the Financial Times Visual Journalism Team  as a starting bench mark when creating a graph.\nOften, the graphical display of the information created for answering a specific question will invite further exploration, which is why it is important to present them in a clear and truthful manner. We should not forget that the sole purpose of data analysis, thus visualisation is to inform and to improve the knowledge. So yes, we should consider very carefully aesthetic appeal and design of the graph we create that could effectively engage with the audience, but should do so by focusing above all on accuracy, depth and clarity of its conveying information.\nIdentify encoding Let us play the game ‚ÄòIdentify encoding!‚Äô. Converse with each other and make a list of graphical forms and the type of encodings used in each of the following visualisation:\n1) Visualisation: DESI\n2) Visualisation: DESI Report 2019 - Human Capital\n3) Visualisation: DESI Report 2019 - Human Capital\n4) Click on Visualisation: Gapminder Bubble Chart\n\n5) Click on Visualisation: Gapminder World Population\n\n6) Click on Visualisation: Periodic Table\n\nClick on this Visualisation too: A Periodic Table of visualisation methods\n7) Click on Visualisation: Clinton Email Network\n\nThe Complexity: effective visual design The most important to remember when creating a graphical is to present data clearly and truthfully. The choice of the scale on the chart should show the differences in the data and communicates the range of values accurately. Any statistical summary displayed on the chart should be presented clearly with the source of information and statistics used to calculate the more complex figures.\nHere are some most obvious issues you need to pay attention to when designing an effective graph:\n1) Choose a scale for your charts that strikes a balance between demonstrating trends clearly and conveying the scale of the original dataset. The chart does not need to begin at 0 in order to establish a meaningful baseline if another logical starting point exists. Dual-Scaled Axes in Graphs\nThe choice of scale should allow for better accuracy that reader can draw about the information displayed on the chart.\nHave a look at What to consider when creating a line chart blog post by Lisa Charlotte Rost, where you will also find the links to some more interesting posts on this topic.\n 2) Emphasise what\u0026rsquo;s important: Identify what is the key information you are trying to communicate and think of the most effective format to do so as graphs can help you to express complex data in a simple format. Displaying an important item in a different colour is an easy way to draw attention to a point-making value.\nSometimes it might be effective to pull the key information from a chart into a separathe graphs and to present them in parralell.\nKeep in mind that taming the information from the visual display should not be difficult.\n 3) Declutter the chart ‚Äì keep it simple as effective visualisations show the data to tell the story. Graphs would not look better by piling on the information and bombarding on with fancy ‚Äòviz‚Äô skills. Effective data visualisation is a delicate balancing act between form and function. Keep the focus on the important points by reducing unnecessay visual stimulus.\nIntegrate the text with the graphs only if necessary to help better convey information effigy.\nIn this presentation Alberto Cairo illustrates the importance of a good choice of the format used to visualy explain the main story: \u0026ldquo;How Music Preferences Have Changed in Two Decades\u0026rdquo;.\nWhen creating a data visualisation, think about the specific information that you want your data to convey, or the outcome that you want to achieve. Keep it simple and remove any unnecessary elements that could convolute your central point, bombarding an audience with too much data will likely leave them doubtful and confused.\n Going beyond obvious When creating a graphical display focus on best practices and explore your own personal style. Build a foundation of exploring and summurising a set of numbers and identifying the key feature within the data that would help you in presenting you visual data story.\nDual Axis Charts There is a belief that charts with two different y-axes make it hard for most people to read and to make right conclusions about two data sets. Having a secondary y-axis often creates a confusion as it is not clear which data to read against which axis. The main danger of dual axis charts is that they‚Äôre not intuitive. There are many people who are opposing them as they often create confusion and assum correlation when there is none. Stephen Few however, has wrote a well-argued paper in which he carefully presents issues one needs to take into consideration when wanting to use them. You need to judge for yourself if you are going to be fun or not of dual y-axis charts. It should certainly depend on your judgment of its suitability for conveying your graphical story telling as with any other graphical format.\nInteractive Charts Interactivity allows the viewer to engage with data in ways impossible by static graphs. One of the key benefits of interactive data visualizations is their flexibility in allowing further manipulation and exploration of the data used. By enabling concentrated focus by ‚Äòzoom in‚Äô facility it makes discovery of the seemingly small facts in a big story easy and engaging as users are invited to pose additional questions and come up to the new findings. Interactive graphical story telling is a rich and powerful tool for displaying data features as it enables viewers to dive into the data story as little or as much as they‚Äôre interested.\n\nGraphical Displays in R: from where to start? R is a great tool for data visualisation not just for people who want to develop understanding about their data using graphs, but nayone with need to produce high-quality and effective graphics to enhance their reports, web pages, or other documents.\nTake a look at The R Graph Gallery that provides an extensive collection of charts made in R together with the code.\nThere are several clasic books on drwing graphics in R, such as:\n ggplot2 [Wickham, 2009] Lattice [Sarkar, 2008] and R Graphics [Murrell, 2011].  If you would like to get more serious and systematic in your learning about graphical data analysis with R Graphical Data Analysis with R book by Antony Unwin is a great place to start. The R code for every graphic and analysis in the book is available from the book\u0026rsquo;s website.\nData Visualization with R by Rob Kabacoff is another good book about graphical displayes in R. It gives a systematic overview for creating graphical display in R starting with a brief introduction to R and following by a comprehensive list of graphical forms commanly used in statistical modelling, geospatial mapping and finishing with interactive plots.\nBack in 2012 there was a very engaging academic discussion between Andrew Gelman, Anthony Unwin, Stephen Few, Paul Murrell, Hadley Wickham and Robert Kosara about visualisation and infographics. Robert compiles all the discussion posts in this blog post that makes riveting reading.\nPractise! Gaining experience in interpreting graphics and drawing your own data displays is the most effective way of becaming a data viz.\n YOUR TURN üëá 1) Go to the portals with open data (global: gapminder, national: office of national statistics, or local) and see if you can find a data that is interesting for you to explore. Write down what interesting features you are expecting to see and suggest type of the visualistions that could be used to illustrate them.\n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/",
	"title": "data challenge with R",
	"tags": [],
	"description": "",
	"content": " data challenge with R Tatjana Kecojevic, SisterAnalyst  \u0026ldquo;The greatest value of a picture is when it forces us to notice what we never expected to see\u0026hellip; Let us take boring flat data and bring it to life through visualization.\u0026rdquo;\nJohn Tukey\n  Open data is data in digital format that is made available to everyone. To access it without the need for negotiation or obstruction at any time is a fundamental feature of the democratisation of digital data. In order for this to happen it requires public authorities to make data available to everyone, in particular those with the necessary skills and knowledge to reveal and interpret the story within complex details. A greater understanding of the data empowers citizens to make more informed decisions in processes that directly affect their lives.\n  R together with RStudio is the best data science tool! It is open source and free software that is available to anyone with a desire to discover, learn, explore, experience, expand and share the algorithms of their data science journey.\n To book your place go here üëâüì© This workshop is a part of The Autumn Data School program organised as a part of the Open Data - Open Opportunities project.\nYou can download the Autumn Data School program from here üëá In partnership with \n¬© 2019 Tatjana Kecojevic\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]